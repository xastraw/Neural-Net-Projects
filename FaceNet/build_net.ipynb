{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72e0f263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train images: 4631\n",
      "Total validation images: 1122\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.amp import autocast, GradScaler\n",
    "import torchvision.transforms as T\n",
    "from utils.import_data import WiderFaceDataset, TRANSFORM, TRAIN_ROOT, TRAIN_ANN_FILE, TEST_ROOT, VAL_ROOT, VAL_ANN_FILE\n",
    "#from utils.anchors import AnchorMatcher, AnchorGenerator, box_nms, compute_loss_with_anchors\n",
    "from utils.anchors import AnchorGeneratorSingle, compute_loss_single_face, compute_iou, decode_predictions\n",
    "\n",
    "#custom collate function to seperate the boxes from the images\n",
    "def custom_collate_fn(batch):\n",
    "    images, targets = zip(*batch)  # unzip the batch\n",
    "    return images, targets\n",
    "\n",
    "# Create the dataset\n",
    "dataset = WiderFaceDataset(\n",
    "    root_dir=TRAIN_ROOT, \n",
    "    annotation_file=TRAIN_ANN_FILE, \n",
    "    img_size=224, \n",
    "    transform=TRANSFORM,\n",
    "    single_face_only=True #if true, thirds the total images\n",
    "    )\n",
    "\n",
    "# Create a dataloader\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=64, #64 runs quickest on desktop\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    #use collate here because our data has the images but also the boxes and number of boxes\n",
    "    collate_fn = custom_collate_fn\n",
    "    )\n",
    "\n",
    "\n",
    "#load validation pictures\n",
    "val_dataset = WiderFaceDataset(\n",
    "    root_dir=VAL_ROOT, \n",
    "    annotation_file=VAL_ANN_FILE, \n",
    "    img_size=224, \n",
    "    transform=TRANSFORM,\n",
    "    single_face_only=True\n",
    "    )\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    collate_fn = custom_collate_fn\n",
    "    )\n",
    "\n",
    "print(f\"Total train images: {len(dataset)}\")\n",
    "print(f\"Total validation images: {len(val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79358bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes the intersection over union (IoU) for a single face image. On a scale of 0-1, a higher number means the boxes are more accurate\n",
    "def anchor_validate_iou(net, val_loader, device, anchors, conf_thresh=0.01, img_size=224):\n",
    "    net.eval()\n",
    "    iou_sum = 0.0\n",
    "    count = 0\n",
    "    \n",
    "    # Track how many predictions we're getting\n",
    "    total_samples = 0\n",
    "    samples_with_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in val_loader:\n",
    "            images = torch.stack(images).to(device)\n",
    "            outputs = net(images)\n",
    "\n",
    "            B, C, H, W = outputs.shape\n",
    "            outputs = outputs.permute(0, 2, 3, 1).reshape(B, -1, 5)\n",
    "\n",
    "            for i in range(B):\n",
    "                total_samples += 1\n",
    "                pred = outputs[i]  # [H*W, 5]\n",
    "\n",
    "                # Decode predictions properly\n",
    "                pred_offsets = pred[:, :4]\n",
    "                pred_scores = torch.sigmoid(pred[:, 4])\n",
    "                \n",
    "                # Decode to actual boxes\n",
    "                pred_boxes = decode_predictions(pred_offsets, anchors)\n",
    "\n",
    "                # Confidence filter\n",
    "                keep = pred_scores > conf_thresh\n",
    "                if keep.sum() == 0:\n",
    "                    continue\n",
    "\n",
    "                samples_with_predictions += 1\n",
    "                pred_boxes = pred_boxes[keep]\n",
    "                pred_scores = pred_scores[keep]\n",
    "\n",
    "                # Pick best box\n",
    "                best_idx = pred_scores.argmax()\n",
    "                pred_box = pred_boxes[best_idx]\n",
    "\n",
    "                # Get GT box\n",
    "                gt_boxes = targets[i]['boxes']\n",
    "                if gt_boxes.shape[0] == 0:\n",
    "                    continue\n",
    "                gt_box = gt_boxes[0].to(device)\n",
    "\n",
    "                # Compute IoU\n",
    "                iou = compute_iou(pred_box.unsqueeze(0), gt_box.unsqueeze(0))\n",
    "                iou_sum += iou.item()\n",
    "                count += 1\n",
    "\n",
    "    net.train()\n",
    "    return iou_sum / max(count, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dea22ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDetectionNet(nn.Module):\n",
    "    def __init__(self, num_anchors=1):\n",
    "        \"\"\"\n",
    "        num_anchors: the number of anchor sizes (3: [32, 64, 96])\n",
    "        kernel_size is the size of the box we pass over each img to extract the features, exactly like tf (3,3,3)\n",
    "        \"\"\"\n",
    "        super(FaceDetectionNet, self).__init__()\n",
    "\n",
    "        #Backbone (feature extractor)\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),  # RGB input\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "\n",
    "        # Detection head\n",
    "        # Predict bounding boxes + confidence\n",
    "        # Output channels = num_anchors * 5 (x, y, w, h, conf)\n",
    "        self.det_head = nn.Conv2d(128, num_anchors * 5, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return self.det_head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "937d7838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face size stats:\n",
      "  Min: 3.8\n",
      "  Max: 175.4\n",
      "  Mean: 50.3\n",
      "  Median: 42.4\n",
      "\n",
      "Your current anchor size: 32\n",
      "Recommended anchor size: 42\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "face_sizes = []\n",
    "for i in range(min(100, len(dataset))):  # Check first 100 images\n",
    "    _, target = dataset[i]\n",
    "    box = target['boxes'][0]\n",
    "    width = box[2] - box[0]\n",
    "    height = box[3] - box[1]\n",
    "    avg_size = (width + height) / 2\n",
    "    face_sizes.append(avg_size.item())\n",
    "\n",
    "print(f\"Face size stats:\")\n",
    "print(f\"  Min: {min(face_sizes):.1f}\")\n",
    "print(f\"  Max: {max(face_sizes):.1f}\")\n",
    "print(f\"  Mean: {np.mean(face_sizes):.1f}\")\n",
    "print(f\"  Median: {np.median(face_sizes):.1f}\")\n",
    "print(f\"\\nYour current anchor size: 32\")\n",
    "print(f\"Recommended anchor size: {int(np.median(face_sizes))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950c2ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on: cuda\n",
      "Loaded checkpoint from epoch 10\n",
      "Epoch [11/120] - Train Loss: 1.6696 - Avg IoU: 43.186% - Reg: 0.0517 - Conf: 0.4673 -  Loss: 1.5894 - Time taken: 01:21.42\n",
      "Epoch [12/120] - Train Loss: 1.6190 - Avg IoU: 45.423% - Reg: 0.1061 - Conf: 0.4733 -  Loss: 1.9121 - Time taken: 01:21.70\n",
      "Epoch [13/120] - Train Loss: 1.6412 - Avg IoU: 45.736% - Reg: 0.0426 - Conf: 0.4710 -  Loss: 1.5989 - Time taken: 01:21.64\n",
      "Epoch [14/120] - Train Loss: 1.6420 - Avg IoU: 38.380% - Reg: 0.0483 - Conf: 0.4730 -  Loss: 1.4821 - Time taken: 01:27.92\n",
      "Epoch [15/120] - Train Loss: 1.6226 - Avg IoU: 42.159% - Reg: 0.1069 - Conf: 0.4668 -  Loss: 1.8444 - Time taken: 01:27.20 - Val IoU: 14.946%\n",
      "Epoch [16/120] - Train Loss: 1.6083 - Avg IoU: 55.715% - Reg: 0.1347 - Conf: 0.4730 -  Loss: 2.2606 - Time taken: 01:27.93\n",
      "Epoch [17/120] - Train Loss: 1.5857 - Avg IoU: 42.447% - Reg: 0.0496 - Conf: 0.4652 -  Loss: 1.5622 - Time taken: 01:27.69\n",
      "Epoch [18/120] - Train Loss: 1.5639 - Avg IoU: 40.220% - Reg: 0.0457 - Conf: 0.4648 -  Loss: 1.4975 - Time taken: 01:27.88\n",
      "Epoch [19/120] - Train Loss: 1.5485 - Avg IoU: 38.662% - Reg: 0.0330 - Conf: 0.4722 -  Loss: 1.4106 - Time taken: 01:28.02\n",
      "Epoch [20/120] - Train Loss: 1.5279 - Avg IoU: 36.984% - Reg: 0.0264 - Conf: 0.4692 -  Loss: 1.3411 - Time taken: 01:27.11 - Val IoU: 14.248%\n",
      "Epoch [21/120] - Train Loss: 1.5036 - Avg IoU: 31.410% - Reg: 0.0152 - Conf: 0.4673 -  Loss: 1.1716 - Time taken: 01:27.98\n",
      "Epoch [22/120] - Train Loss: 1.4918 - Avg IoU: 41.893% - Reg: 0.0516 - Conf: 0.4671 -  Loss: 1.5629 - Time taken: 01:27.38\n",
      "Epoch [23/120] - Train Loss: 1.4762 - Avg IoU: 34.300% - Reg: 0.0425 - Conf: 0.4597 -  Loss: 1.3583 - Time taken: 01:27.02\n",
      "Epoch [24/120] - Train Loss: 1.4621 - Avg IoU: 45.714% - Reg: 0.0613 - Conf: 0.4661 -  Loss: 1.6868 - Time taken: 01:27.46\n",
      "Epoch [25/120] - Train Loss: 1.4393 - Avg IoU: 33.871% - Reg: 0.0312 - Conf: 0.4612 -  Loss: 1.2944 - Time taken: 01:27.62 - Val IoU: 17.369%\n",
      "Epoch [26/120] - Train Loss: 1.4336 - Avg IoU: 39.488% - Reg: 0.0519 - Conf: 0.4669 -  Loss: 1.5163 - Time taken: 01:27.04\n",
      "Epoch [27/120] - Train Loss: 1.4233 - Avg IoU: 38.109% - Reg: 0.0228 - Conf: 0.4647 -  Loss: 1.3409 - Time taken: 01:27.67\n",
      "Epoch [28/120] - Train Loss: 1.4157 - Avg IoU: 39.600% - Reg: 0.0575 - Conf: 0.4636 -  Loss: 1.5432 - Time taken: 01:27.30\n",
      "Epoch [29/120] - Train Loss: 1.3993 - Avg IoU: 45.871% - Reg: 0.1001 - Conf: 0.4662 -  Loss: 1.8838 - Time taken: 01:27.43\n",
      "Epoch [30/120] - Train Loss: 1.4178 - Avg IoU: 38.749% - Reg: 0.0168 - Conf: 0.4650 -  Loss: 1.3238 - Time taken: 01:27.36 - Val IoU: 17.831%\n",
      "Epoch [31/120] - Train Loss: 1.4694 - Avg IoU: 39.388% - Reg: 0.0239 - Conf: 0.4671 -  Loss: 1.3745 - Time taken: 01:27.31\n",
      "Epoch [32/120] - Train Loss: 1.4584 - Avg IoU: 35.117% - Reg: 0.0233 - Conf: 0.4587 -  Loss: 1.2774 - Time taken: 01:27.19\n",
      "Epoch [33/120] - Train Loss: 1.4398 - Avg IoU: 40.540% - Reg: 0.0666 - Conf: 0.4659 -  Loss: 1.6098 - Time taken: 01:26.96\n",
      "Epoch [34/120] - Train Loss: 1.3396 - Avg IoU: 27.958% - Reg: 0.0109 - Conf: 0.4596 -  Loss: 1.0733 - Time taken: 01:27.37\n",
      "Epoch [35/120] - Train Loss: 1.2760 - Avg IoU: 26.688% - Reg: 0.0100 - Conf: 0.4614 -  Loss: 1.0451 - Time taken: 01:27.68 - Val IoU: 18.478%\n",
      "Epoch [36/120] - Train Loss: 1.2568 - Avg IoU: 34.217% - Reg: 0.0394 - Conf: 0.4587 -  Loss: 1.3400 - Time taken: 01:27.38\n",
      "Epoch [37/120] - Train Loss: 1.2352 - Avg IoU: 31.222% - Reg: 0.0180 - Conf: 0.4593 -  Loss: 1.1737 - Time taken: 01:27.15\n",
      "Epoch [38/120] - Train Loss: 1.2246 - Avg IoU: 32.619% - Reg: 0.0152 - Conf: 0.4613 -  Loss: 1.1896 - Time taken: 01:27.12\n",
      "Epoch [39/120] - Train Loss: 1.2094 - Avg IoU: 29.862% - Reg: 0.0376 - Conf: 0.4581 -  Loss: 1.2432 - Time taken: 01:27.48\n",
      "Epoch [40/120] - Train Loss: 1.2030 - Avg IoU: 29.090% - Reg: 0.0245 - Conf: 0.4636 -  Loss: 1.1681 - Time taken: 01:27.55 - Val IoU: 18.703%\n",
      "Epoch [41/120] - Train Loss: 1.1932 - Avg IoU: 29.410% - Reg: 0.0248 - Conf: 0.4635 -  Loss: 1.1755 - Time taken: 01:29.07\n",
      "Epoch [42/120] - Train Loss: 1.1874 - Avg IoU: 29.472% - Reg: 0.0150 - Conf: 0.4590 -  Loss: 1.1235 - Time taken: 01:26.90\n",
      "Epoch [43/120] - Train Loss: 1.1778 - Avg IoU: 35.911% - Reg: 0.0459 - Conf: 0.4594 -  Loss: 1.4072 - Time taken: 01:26.96\n",
      "Epoch [44/120] - Train Loss: 1.1791 - Avg IoU: 34.873% - Reg: 0.0489 - Conf: 0.4597 -  Loss: 1.4019 - Time taken: 01:27.37\n",
      "Epoch [45/120] - Train Loss: 1.1662 - Avg IoU: 24.373% - Reg: 0.0079 - Conf: 0.4594 -  Loss: 0.9861 - Time taken: 01:26.98 - Val IoU: 19.220%\n",
      "Epoch [46/120] - Train Loss: 1.1658 - Avg IoU: 29.390% - Reg: 0.0224 - Conf: 0.4613 -  Loss: 1.1612 - Time taken: 01:26.74\n",
      "Epoch [47/120] - Train Loss: 1.1631 - Avg IoU: 24.193% - Reg: 0.0210 - Conf: 0.4578 -  Loss: 1.0465 - Time taken: 01:26.81\n",
      "Epoch [48/120] - Train Loss: 1.1576 - Avg IoU: 33.645% - Reg: 0.0761 - Conf: 0.4569 -  Loss: 1.5103 - Time taken: 01:27.06\n",
      "Epoch [49/120] - Train Loss: 1.1515 - Avg IoU: 24.195% - Reg: 0.0145 - Conf: 0.4636 -  Loss: 1.0202 - Time taken: 01:26.97\n",
      "Epoch [50/120] - Train Loss: 1.1347 - Avg IoU: 25.201% - Reg: 0.0282 - Conf: 0.4599 -  Loss: 1.1049 - Time taken: 01:26.52 - Val IoU: 19.618%\n",
      "Epoch [51/120] - Train Loss: 1.1438 - Avg IoU: 28.470% - Reg: 0.0439 - Conf: 0.4618 -  Loss: 1.2508 - Time taken: 01:27.07\n",
      "Epoch [52/120] - Train Loss: 1.1395 - Avg IoU: 26.933% - Reg: 0.0325 - Conf: 0.4602 -  Loss: 1.1614 - Time taken: 01:26.59\n",
      "Epoch [53/120] - Train Loss: 1.1316 - Avg IoU: 29.821% - Reg: 0.0151 - Conf: 0.4602 -  Loss: 1.1324 - Time taken: 01:27.06\n",
      "Epoch [54/120] - Train Loss: 1.1299 - Avg IoU: 23.172% - Reg: 0.0108 - Conf: 0.4568 -  Loss: 0.9743 - Time taken: 01:27.12\n",
      "Epoch [55/120] - Train Loss: 1.1295 - Avg IoU: 31.328% - Reg: 0.0226 - Conf: 0.4608 -  Loss: 1.2002 - Time taken: 01:26.71 - Val IoU: 19.185%\n",
      "Epoch [56/120] - Train Loss: 1.1170 - Avg IoU: 24.188% - Reg: 0.0066 - Conf: 0.4584 -  Loss: 0.9754 - Time taken: 01:26.90\n",
      "Epoch [57/120] - Train Loss: 1.1155 - Avg IoU: 26.154% - Reg: 0.0254 - Conf: 0.4577 -  Loss: 1.1077 - Time taken: 01:27.20\n",
      "Epoch [58/120] - Train Loss: 1.1224 - Avg IoU: 36.995% - Reg: 0.0670 - Conf: 0.4601 -  Loss: 1.5352 - Time taken: 01:27.18\n",
      "Epoch [59/120] - Train Loss: 1.1201 - Avg IoU: 46.125% - Reg: 0.0959 - Conf: 0.4602 -  Loss: 1.8621 - Time taken: 01:27.33\n",
      "Epoch [60/120] - Train Loss: 1.1080 - Avg IoU: 25.102% - Reg: 0.0147 - Conf: 0.4573 -  Loss: 1.0330 - Time taken: 01:26.81 - Val IoU: 19.299%\n",
      "Epoch [61/120] - Train Loss: 1.1037 - Avg IoU: 27.652% - Reg: 0.0273 - Conf: 0.4613 -  Loss: 1.1506 - Time taken: 01:27.35\n",
      "Epoch [62/120] - Train Loss: 1.0977 - Avg IoU: 20.027% - Reg: 0.0110 - Conf: 0.4572 -  Loss: 0.9129 - Time taken: 01:27.04\n",
      "Epoch [63/120] - Train Loss: 1.1063 - Avg IoU: 33.615% - Reg: 0.0384 - Conf: 0.4581 -  Loss: 1.3222 - Time taken: 01:27.30\n",
      "Epoch [64/120] - Train Loss: 1.0975 - Avg IoU: 23.480% - Reg: 0.0139 - Conf: 0.4584 -  Loss: 0.9976 - Time taken: 01:26.36\n",
      "Epoch [65/120] - Train Loss: 1.0932 - Avg IoU: 25.564% - Reg: 0.0106 - Conf: 0.4574 -  Loss: 1.0215 - Time taken: 01:26.86 - Val IoU: 18.827%\n",
      "Epoch [66/120] - Train Loss: 1.0866 - Avg IoU: 29.120% - Reg: 0.0317 - Conf: 0.4568 -  Loss: 1.1975 - Time taken: 01:26.36\n",
      "Epoch [67/120] - Train Loss: 1.0918 - Avg IoU: 24.746% - Reg: 0.0249 - Conf: 0.4585 -  Loss: 1.0777 - Time taken: 01:26.84\n",
      "Epoch [68/120] - Train Loss: 1.0885 - Avg IoU: 26.272% - Reg: 0.0464 - Conf: 0.4553 -  Loss: 1.2124 - Time taken: 01:26.73\n",
      "Epoch [69/120] - Train Loss: 1.0896 - Avg IoU: 27.122% - Reg: 0.0195 - Conf: 0.4610 -  Loss: 1.1011 - Time taken: 01:26.94\n",
      "Epoch [70/120] - Train Loss: 1.0880 - Avg IoU: 25.451% - Reg: 0.0163 - Conf: 0.4555 -  Loss: 1.0458 - Time taken: 01:26.68 - Val IoU: 18.528%\n",
      "Epoch [71/120] - Train Loss: 1.0570 - Avg IoU: 24.106% - Reg: 0.0255 - Conf: 0.4559 -  Loss: 1.0656 - Time taken: 01:27.18\n",
      "Epoch [72/120] - Train Loss: 1.0450 - Avg IoU: 19.368% - Reg: 0.0196 - Conf: 0.4566 -  Loss: 0.9420 - Time taken: 01:26.75\n",
      "Epoch [73/120] - Train Loss: 1.0427 - Avg IoU: 27.116% - Reg: 0.0243 - Conf: 0.4580 -  Loss: 1.1217 - Time taken: 01:27.27\n",
      "Epoch [74/120] - Train Loss: 1.0355 - Avg IoU: 23.100% - Reg: 0.0122 - Conf: 0.4555 -  Loss: 0.9784 - Time taken: 01:26.60\n",
      "Epoch [75/120] - Train Loss: 1.0351 - Avg IoU: 29.298% - Reg: 0.0147 - Conf: 0.4574 -  Loss: 1.1168 - Time taken: 01:26.50 - Val IoU: 19.012%\n",
      "Epoch [76/120] - Train Loss: 1.0372 - Avg IoU: 33.770% - Reg: 0.0267 - Conf: 0.4579 -  Loss: 1.2666 - Time taken: 01:27.24\n",
      "Epoch [77/120] - Train Loss: 1.0379 - Avg IoU: 30.641% - Reg: 0.0592 - Conf: 0.4574 -  Loss: 1.3661 - Time taken: 01:26.97\n",
      "Epoch [78/120] - Train Loss: 1.0365 - Avg IoU: 27.377% - Reg: 0.0554 - Conf: 0.4552 -  Loss: 1.2800 - Time taken: 01:26.33\n",
      "Epoch [79/120] - Train Loss: 1.0351 - Avg IoU: 30.333% - Reg: 0.0489 - Conf: 0.4580 -  Loss: 1.3094 - Time taken: 01:26.64\n",
      "Epoch [80/120] - Train Loss: 1.0214 - Avg IoU: 25.846% - Reg: 0.0351 - Conf: 0.4558 -  Loss: 1.1483 - Time taken: 01:26.69 - Val IoU: 18.736%\n",
      "Epoch [81/120] - Train Loss: 1.0164 - Avg IoU: 27.042% - Reg: 0.0112 - Conf: 0.4569 -  Loss: 1.0535 - Time taken: 01:27.13\n",
      "Epoch [82/120] - Train Loss: 1.0213 - Avg IoU: 28.069% - Reg: 0.0246 - Conf: 0.4562 -  Loss: 1.1408 - Time taken: 01:26.72\n",
      "Epoch [83/120] - Train Loss: 1.0193 - Avg IoU: 15.577% - Reg: 0.0156 - Conf: 0.4555 -  Loss: 0.8450 - Time taken: 01:26.98\n",
      "Epoch [84/120] - Train Loss: 1.0161 - Avg IoU: 20.522% - Reg: 0.0090 - Conf: 0.4571 -  Loss: 0.9124 - Time taken: 01:26.98\n",
      "Epoch [85/120] - Train Loss: 1.0192 - Avg IoU: 29.258% - Reg: 0.0273 - Conf: 0.4559 -  Loss: 1.1776 - Time taken: 01:26.57 - Val IoU: 19.063%\n",
      "Epoch [86/120] - Train Loss: 1.0187 - Avg IoU: 24.268% - Reg: 0.0341 - Conf: 0.4561 -  Loss: 1.1119 - Time taken: 01:26.55\n",
      "Epoch [87/120] - Train Loss: 1.0195 - Avg IoU: 32.239% - Reg: 0.0389 - Conf: 0.4570 -  Loss: 1.2964 - Time taken: 01:26.92\n",
      "Epoch [88/120] - Train Loss: 1.0205 - Avg IoU: 26.856% - Reg: 0.0288 - Conf: 0.4594 -  Loss: 1.1405 - Time taken: 01:27.03\n",
      "Epoch [89/120] - Train Loss: 1.0204 - Avg IoU: 26.117% - Reg: 0.0566 - Conf: 0.4554 -  Loss: 1.2606 - Time taken: 01:26.81\n",
      "Epoch [90/120] - Train Loss: 1.0207 - Avg IoU: 26.044% - Reg: 0.0285 - Conf: 0.4578 -  Loss: 1.1214 - Time taken: 01:27.28 - Val IoU: 18.959%\n",
      "Epoch [91/120] - Train Loss: 1.0182 - Avg IoU: 26.337% - Reg: 0.0348 - Conf: 0.4565 -  Loss: 1.1572 - Time taken: 01:27.06\n",
      "Epoch [92/120] - Train Loss: 1.0215 - Avg IoU: 24.964% - Reg: 0.0475 - Conf: 0.4564 -  Loss: 1.1930 - Time taken: 01:26.68\n",
      "Epoch [93/120] - Train Loss: 1.0138 - Avg IoU: 28.398% - Reg: 0.0200 - Conf: 0.4574 -  Loss: 1.1251 - Time taken: 01:26.47\n",
      "Epoch [94/120] - Train Loss: 1.0114 - Avg IoU: 18.975% - Reg: 0.0112 - Conf: 0.4600 -  Loss: 0.8957 - Time taken: 01:27.26\n",
      "Epoch [95/120] - Train Loss: 1.0136 - Avg IoU: 25.042% - Reg: 0.0254 - Conf: 0.4568 -  Loss: 1.0848 - Time taken: 01:26.93 - Val IoU: 19.164%\n",
      "Epoch [96/120] - Train Loss: 1.0096 - Avg IoU: 22.885% - Reg: 0.0137 - Conf: 0.4592 -  Loss: 0.9854 - Time taken: 01:26.92\n",
      "Epoch [97/120] - Train Loss: 1.0173 - Avg IoU: 23.262% - Reg: 0.0267 - Conf: 0.4563 -  Loss: 1.0552 - Time taken: 01:27.15\n",
      "Epoch [98/120] - Train Loss: 1.0156 - Avg IoU: 24.178% - Reg: 0.0480 - Conf: 0.4576 -  Loss: 1.1812 - Time taken: 01:26.28\n",
      "Epoch [99/120] - Train Loss: 1.0077 - Avg IoU: 21.002% - Reg: 0.0189 - Conf: 0.4563 -  Loss: 0.9708 - Time taken: 01:27.07\n",
      "Epoch [100/120] - Train Loss: 1.0179 - Avg IoU: 28.721% - Reg: 0.0307 - Conf: 0.4563 -  Loss: 1.1843 - Time taken: 01:27.10 - Val IoU: 18.774%\n",
      "Epoch [101/120] - Train Loss: 1.0189 - Avg IoU: 30.483% - Reg: 0.0446 - Conf: 0.4595 -  Loss: 1.2921 - Time taken: 01:27.42\n",
      "Epoch [102/120] - Train Loss: 1.0108 - Avg IoU: 20.923% - Reg: 0.0164 - Conf: 0.4580 -  Loss: 0.9585 - Time taken: 01:26.81\n",
      "Epoch [103/120] - Train Loss: 1.0231 - Avg IoU: 38.725% - Reg: 0.0981 - Conf: 0.4578 -  Loss: 1.7228 - Time taken: 01:26.75\n",
      "Epoch [104/120] - Train Loss: 1.0238 - Avg IoU: 28.413% - Reg: 0.0837 - Conf: 0.4565 -  Loss: 1.4431 - Time taken: 01:26.86\n",
      "Epoch [105/120] - Train Loss: 1.0111 - Avg IoU: 27.975% - Reg: 0.0269 - Conf: 0.4565 -  Loss: 1.1506 - Time taken: 01:27.14 - Val IoU: 18.988%\n",
      "Epoch [106/120] - Train Loss: 1.0096 - Avg IoU: 27.014% - Reg: 0.0326 - Conf: 0.4554 -  Loss: 1.1589 - Time taken: 01:26.90\n",
      "Epoch [107/120] - Train Loss: 1.0061 - Avg IoU: 18.929% - Reg: 0.0074 - Conf: 0.4545 -  Loss: 0.8701 - Time taken: 01:26.59\n",
      "Epoch [108/120] - Train Loss: 1.0162 - Avg IoU: 23.404% - Reg: 0.0102 - Conf: 0.4562 -  Loss: 0.9752 - Time taken: 01:26.96\n",
      "Epoch [109/120] - Train Loss: 1.0179 - Avg IoU: 20.292% - Reg: 0.0076 - Conf: 0.4575 -  Loss: 0.9016 - Time taken: 01:27.01\n",
      "Epoch [110/120] - Train Loss: 1.0128 - Avg IoU: 20.425% - Reg: 0.0183 - Conf: 0.4576 -  Loss: 0.9575 - Time taken: 01:27.06 - Val IoU: 18.909%\n",
      "Epoch [111/120] - Train Loss: 1.0137 - Avg IoU: 23.289% - Reg: 0.0486 - Conf: 0.4561 -  Loss: 1.1647 - Time taken: 01:27.43\n",
      "Epoch [112/120] - Train Loss: 1.0173 - Avg IoU: 31.015% - Reg: 0.0432 - Conf: 0.4572 -  Loss: 1.2934 - Time taken: 01:26.99\n",
      "Epoch [113/120] - Train Loss: 1.0148 - Avg IoU: 23.281% - Reg: 0.0441 - Conf: 0.4573 -  Loss: 1.1436 - Time taken: 01:27.15\n",
      "Epoch [114/120] - Train Loss: 1.0092 - Avg IoU: 20.121% - Reg: 0.0083 - Conf: 0.4591 -  Loss: 0.9031 - Time taken: 01:27.00\n",
      "Epoch [115/120] - Train Loss: 1.0127 - Avg IoU: 20.553% - Reg: 0.0200 - Conf: 0.4558 -  Loss: 0.9667 - Time taken: 01:26.66 - Val IoU: 18.568%\n",
      "Epoch [116/120] - Train Loss: 1.0076 - Avg IoU: 21.915% - Reg: 0.0048 - Conf: 0.4554 -  Loss: 0.9176 - Time taken: 01:26.72\n",
      "Epoch [117/120] - Train Loss: 1.0118 - Avg IoU: 23.771% - Reg: 0.0108 - Conf: 0.4572 -  Loss: 0.9864 - Time taken: 01:27.06\n",
      "Epoch [118/120] - Train Loss: 1.0142 - Avg IoU: 25.769% - Reg: 0.0257 - Conf: 0.4572 -  Loss: 1.1013 - Time taken: 01:27.17\n",
      "Epoch [119/120] - Train Loss: 1.0167 - Avg IoU: 21.928% - Reg: 0.0287 - Conf: 0.4543 -  Loss: 1.0366 - Time taken: 01:26.58\n",
      "Epoch [120/120] - Train Loss: 1.0195 - Avg IoU: 27.099% - Reg: 0.0458 - Conf: 0.4574 -  Loss: 1.2282 - Time taken: 01:26.96 - Val IoU: 18.758%\n"
     ]
    }
   ],
   "source": [
    "#Training Loop\n",
    "start_epoch = 0\n",
    "num_epochs = 120\n",
    "loadLastCheckpoint = True\n",
    "checkpoint_rate = 10\n",
    "validationRate = 5\n",
    "net = FaceDetectionNet()\n",
    "\n",
    "#moves all the info to the gpu (cuda) if it can, if not it keeps it on the cpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Training on: {device}\") \n",
    "net = net.to(device)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0005)\n",
    "scaler = GradScaler()\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.25, patience=3\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize anchor generator\n",
    "anchor_gen = AnchorGeneratorSingle(anchor_size=42)\n",
    "anchors = anchor_gen.generate(feature_h=28, feature_w=28, stride=8, device=device)\n",
    "\n",
    "\n",
    "if loadLastCheckpoint:\n",
    "    # Check for existing checkpoints and load the latest one\n",
    "    checkpoint_files = [f for f in os.listdir(\"checkpoints\") if f.startswith(\"faceNet_checkpoint\")]\n",
    "    if checkpoint_files:\n",
    "        # Sort by epoch number and get the latest\n",
    "        latest_checkpoint = sorted(checkpoint_files, key=lambda x: int(x.split('checkpoint')[1].split('.')[0]))[-1]\n",
    "        checkpoint_path = os.path.join(\"checkpoints\", latest_checkpoint)\n",
    "        \n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        net.load_state_dict(checkpoint[\"model_state\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
    "        start_epoch = checkpoint[\"epoch\"] + 1\n",
    "        print(f\"Loaded checkpoint from epoch {checkpoint['epoch'] + 1}\")\n",
    "\n",
    "timer_start = time.time()\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    net.train() # sets the net to training mode\n",
    "    epoch_loss = 0\n",
    "\n",
    "    #with pytorch it wont let you pass the entire dataset to the net at once so you have to send it in batches (thats what the batch size is for)\n",
    "    for images, targets in dataloader:\n",
    "        images = torch.stack(images).to(device)    # shape: [batch_size, 3, 224, 224]\n",
    "        #boxes = [t['boxes'].to(device) for t in targets]  # move each image's boxes to GPU\n",
    "\n",
    "        optimizer.zero_grad()               # resets gradients\n",
    "        with autocast('cuda'):  # autocast lets you use mixed precision for faster training on nvidia gpu, uses 16bit floating points instead of 32\n",
    "            outputs = net(images)\n",
    "            loss, avg_reg, avg_conf, avg_iou = compute_loss_single_face(outputs, targets, anchors, stride=8)\n",
    "            \n",
    "        scaler.scale(loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0) # Gradient clipping to prevent exploding gradients\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        epoch_loss += loss.item()  # accumulate batch loss\n",
    "\n",
    "    #timer to see how long each epoch takes\n",
    "    timer_end = time.time()\n",
    "    length = timer_end-timer_start\n",
    "    mins, secs = divmod(length, 60)\n",
    "\n",
    "    \n",
    "    avg_epoch_loss = epoch_loss / len(dataloader)\n",
    "    scheduler.step(avg_epoch_loss) #reduces the learning rate once the loss has stopped moving for 3 consecutive epochs\n",
    "\n",
    "    if (epoch+1) % validationRate == 0:\n",
    "        val_iou = anchor_validate_iou(net=net, val_loader=val_loader, device=device, anchors=anchors)\n",
    "        torch.cuda.empty_cache() #free up the cache on the gpu after checking iou\n",
    "\n",
    "        print(\n",
    "        f\"Epoch [{epoch+1}/{num_epochs}]\"\n",
    "        f\" - Train Loss: {avg_epoch_loss:.4f}\"\n",
    "        f\" - Avg IoU: {(avg_iou*100):.3f}%\"\n",
    "        f\" - Reg: {avg_reg:.4f}\"\n",
    "        f\" - Conf: {avg_conf:.4f}\"\n",
    "        f\" -  Loss: {loss.item():.4f}\"\n",
    "        f\" - Time taken: 0{int(mins)}:{secs:.2f}\"\n",
    "        f\" - Val IoU: {(val_iou*100):.3f}%\"\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{num_epochs}]\"\n",
    "            f\" - Train Loss: {avg_epoch_loss:.4f}\"\n",
    "            f\" - Avg IoU: {(avg_iou*100):.3f}%\"\n",
    "            f\" - Reg: {avg_reg:.4f}\"\n",
    "            f\" - Conf: {avg_conf:.4f}\"\n",
    "            f\" -  Loss: {loss.item():.4f}\"\n",
    "            f\" - Time taken: 0{int(mins)}:{secs:.2f}\")\n",
    "    \n",
    "\n",
    "    timer_start = time.time()\n",
    "\n",
    "    #saves a checkpoint\n",
    "    if (epoch+1) % checkpoint_rate == 0:\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state\": net.state_dict(),\n",
    "            \"optimizer_state\": optimizer.state_dict(),\n",
    "            \"loss\": avg_epoch_loss\n",
    "        }, f\"checkpoints/faceNet_checkpoint{epoch+1}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f178e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-create the model architecture\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "loadedNet = FaceDetectionNet()\n",
    "checkpoint_path = os.path.join(\"checkpoints\", \"faceNet_checkpoint30.pth\")\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "loadedNet.load_state_dict(checkpoint[\"model_state\"])\n",
    "loadedNet.to(device)\n",
    "\n",
    "# VERY IMPORTANT for inference\n",
    "loadedNet.eval()\n",
    "iou_avg = anchor_validate_iou(net=loadedNet, val_loader=val_loader, device=device)\n",
    "print(f\"{(iou_avg*100):.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b876332",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "\n",
    "#img = \"43_Row_Boat_Canoe_43_37.jpg\"\n",
    "#img_path = \"C:\\\\Code\\\\Neural Net Projects\\\\FaceNet\\\\Dataset\\\\WIDER_test\\\\images\\\\43--Row_Boat\\\\\" + img\n",
    "\n",
    "img = \"0_Parade_Parade_0_1020.jpg\"\n",
    "\n",
    "img_path = \"C:\\\\Code\\\\Neural Net Projects\\\\FaceNet\\\\Dataset\\\\WIDER_test\\\\images\\\\0--Parade\\\\\" + img\n",
    "def test_net(image):\n",
    "    #load image w/ preprocessing\n",
    "    \n",
    "    orig_w, orig_h = image.size  # save original size\n",
    "\n",
    "    image_tensor = TRANSFORM(image)   # same TRANSFORM as training\n",
    "    image_tensor = image_tensor.unsqueeze(0).to(device)  # [1, 3, 224, 224]\n",
    "\n",
    "    #forward pass w/ no gradient\n",
    "    with torch.no_grad():\n",
    "        outputs = loadedNet(image_tensor)\n",
    "\n",
    "    #decode the predicitions into boxes and scores\n",
    "    pred = outputs[0]  # [1, 5, 28, 28]\n",
    "\n",
    "    # Flatten\n",
    "    pred = pred.view(5, -1).permute(1, 0)  # [28*28, 5]\n",
    "\n",
    "    pred_boxes = pred[:, :4] * 224.0   # undo normalization\n",
    "    pred_scores = torch.sigmoid(pred[:, 4])\n",
    "\n",
    "    #apply confidence threshhold and nms (non max suppresion)\n",
    "    CONF_THRESH = 0.5\n",
    "    NMS_THRESH = 0.4\n",
    "\n",
    "    keep = pred_scores > CONF_THRESH\n",
    "    pred_boxes = pred_boxes[keep]\n",
    "    pred_scores = pred_scores[keep]\n",
    "\n",
    "    if pred_boxes.shape[0] > 0:\n",
    "        keep_idx = box_nms(pred_boxes, pred_scores, iou_threshold=NMS_THRESH)\n",
    "        pred_boxes = pred_boxes[keep_idx]\n",
    "        pred_scores = pred_scores[keep_idx]\n",
    "\n",
    "\n",
    "    #scale boxes back into original image\n",
    "    scale_x = orig_w / 224\n",
    "    scale_y = orig_h / 224\n",
    "\n",
    "    pred_boxes[:, [0, 2]] *= scale_x\n",
    "    pred_boxes[:, [1, 3]] *= scale_y\n",
    "\n",
    "    return pred_boxes, pred_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686bc4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "img = \"0_Parade_Parade_0_1046.jpg\"\n",
    "img_path = \"C:\\\\Code\\\\Neural Net Projects\\\\FaceNet\\\\Dataset\\\\WIDER_test\\\\images\\\\0--Parade\\\\\" + img\n",
    "image = Image.open(img_path).convert(\"RGB\")\n",
    "pred_boxes, pred_scores = test_net(image)\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.imshow(image)\n",
    "\n",
    "for box, score in zip(pred_boxes, pred_scores):\n",
    "    x1, y1, x2, y2 = box.cpu()\n",
    "    rect = patches.Rectangle(\n",
    "        (x1, y1), x2 - x1, y2 - y1,\n",
    "        linewidth=2, edgecolor='blue', facecolor='none'\n",
    "    )\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x1, y1 - 5, f\"Confidence: {score:.2f}\", color='red')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "face-net-clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
