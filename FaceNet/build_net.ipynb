{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72e0f263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train images: 4631\n",
      "Total validation images: 1122\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.amp import autocast, GradScaler\n",
    "import torchvision.transforms as T\n",
    "from utils.import_data import WiderFaceDataset, TRANSFORM, TRAIN_ROOT, TRAIN_ANN_FILE, TEST_ROOT, VAL_ROOT, VAL_ANN_FILE\n",
    "#from utils.anchors import AnchorMatcher, AnchorGenerator, box_nms, compute_loss_with_anchors\n",
    "from utils.anchors import AnchorGeneratorSingle, AnchorGeneratorMultiScale, compute_loss_single_face, anchor_validate_iou\n",
    "\n",
    "#custom collate function to seperate the boxes from the images\n",
    "def custom_collate_fn(batch):\n",
    "    images, targets = zip(*batch)  # unzip the batch\n",
    "    return images, targets\n",
    "\n",
    "# Create the dataset\n",
    "dataset = WiderFaceDataset(\n",
    "    root_dir=TRAIN_ROOT, \n",
    "    annotation_file=TRAIN_ANN_FILE, \n",
    "    img_size=224, \n",
    "    transform=TRANSFORM,\n",
    "    single_face_only=True #if true, thirds the total images\n",
    "    )\n",
    "\n",
    "# Create a dataloader\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=64, #64 runs quickest on desktop\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    #use collate here because our data has the images but also the boxes and number of boxes\n",
    "    collate_fn = custom_collate_fn\n",
    "    )\n",
    "\n",
    "\n",
    "#load validation pictures\n",
    "val_dataset = WiderFaceDataset(\n",
    "    root_dir=VAL_ROOT, \n",
    "    annotation_file=VAL_ANN_FILE, \n",
    "    img_size=224, \n",
    "    transform=TRANSFORM,\n",
    "    single_face_only=True\n",
    "    )\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    collate_fn = custom_collate_fn\n",
    "    )\n",
    "\n",
    "print(f\"Total train images: {len(dataset)}\")\n",
    "print(f\"Total validation images: {len(val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dea22ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDetectionNet(nn.Module):\n",
    "    def __init__(self, num_anchors=1):\n",
    "        \"\"\"\n",
    "        num_anchors: the number of anchor sizes (3: [32, 64, 96])\n",
    "        kernel_size is the size of the box we pass over each img to extract the features, exactly like tf (3,3,3)\n",
    "        \"\"\"\n",
    "        super(FaceDetectionNet, self).__init__()\n",
    "\n",
    "        #Backbone (feature extractor)\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),  # RGB input\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "\n",
    "        # Detection head\n",
    "        # Predict bounding boxes + confidence\n",
    "        # Output channels = num_anchors * 5 (x, y, w, h, conf)\n",
    "        self.det_head = nn.Conv2d(128, num_anchors * 5, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return self.det_head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "937d7838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face size stats:\n",
      "  Min: 3.8\n",
      "  Max: 175.4\n",
      "  Mean: 50.3\n",
      "  Median: 42.4\n",
      "\n",
      "Your current anchor size: 32\n",
      "Recommended anchor size: 42\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "face_sizes = []\n",
    "for i in range(min(100, len(dataset))):  # Check first 100 images\n",
    "    _, target = dataset[i]\n",
    "    box = target['boxes'][0]\n",
    "    width = box[2] - box[0]\n",
    "    height = box[3] - box[1]\n",
    "    avg_size = (width + height) / 2\n",
    "    face_sizes.append(avg_size.item())\n",
    "\n",
    "print(f\"Face size stats:\")\n",
    "print(f\"  Min: {min(face_sizes):.1f}\")\n",
    "print(f\"  Max: {max(face_sizes):.1f}\")\n",
    "print(f\"  Mean: {np.mean(face_sizes):.1f}\")\n",
    "print(f\"  Median: {np.median(face_sizes):.1f}\")\n",
    "print(f\"\\nYour current anchor size: 32\")\n",
    "print(f\"Recommended anchor size: {int(np.median(face_sizes))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "950c2ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on: cuda\n",
      "Epoch [1/100] - Train Loss: 2.1742 - Avg IoU: 58.843% - Reg: 0.0730 - Conf: 0.7162 -  Loss: 1.8179 - Time taken: 01:29.25\n",
      "Epoch [2/100] - Train Loss: 1.6556 - Avg IoU: 50.481% - Reg: 0.0576 - Conf: 0.5868 -  Loss: 1.5169 - Time taken: 01:29.82\n",
      "Epoch [3/100] - Train Loss: 1.5202 - Avg IoU: 45.417% - Reg: 0.0323 - Conf: 0.5350 -  Loss: 1.3131 - Time taken: 01:31.15\n",
      "Epoch [4/100] - Train Loss: 1.4550 - Avg IoU: 46.566% - Reg: 0.0452 - Conf: 0.5109 -  Loss: 1.3449 - Time taken: 01:35.90\n",
      "Epoch [5/100] - Train Loss: 1.4234 - Avg IoU: 44.409% - Reg: 0.0329 - Conf: 0.4895 -  Loss: 1.2543 - Time taken: 01:36.10 - Val IoU: 5.327%\n",
      "Epoch [6/100] - Train Loss: 1.4045 - Avg IoU: 54.129% - Reg: 0.0856 - Conf: 0.4845 -  Loss: 1.5532 - Time taken: 01:35.90\n",
      "Epoch [7/100] - Train Loss: 1.3959 - Avg IoU: 48.830% - Reg: 0.0526 - Conf: 0.4880 -  Loss: 1.3781 - Time taken: 01:35.88\n",
      "Epoch [8/100] - Train Loss: 1.3875 - Avg IoU: 53.296% - Reg: 0.0681 - Conf: 0.4915 -  Loss: 1.4953 - Time taken: 01:35.59\n",
      "Epoch [9/100] - Train Loss: 1.3774 - Avg IoU: 48.852% - Reg: 0.0543 - Conf: 0.4858 -  Loss: 1.3814 - Time taken: 01:35.84\n",
      "Epoch [10/100] - Train Loss: 1.3719 - Avg IoU: 52.470% - Reg: 0.0653 - Conf: 0.4817 -  Loss: 1.4647 - Time taken: 01:35.34 - Val IoU: 6.202%\n",
      "Epoch [11/100] - Train Loss: 1.3654 - Avg IoU: 46.122% - Reg: 0.0455 - Conf: 0.4784 -  Loss: 1.3068 - Time taken: 01:36.00\n",
      "Epoch [12/100] - Train Loss: 1.3580 - Avg IoU: 52.026% - Reg: 0.0803 - Conf: 0.4875 -  Loss: 1.5089 - Time taken: 01:35.89\n",
      "Epoch [13/100] - Train Loss: 1.3545 - Avg IoU: 44.494% - Reg: 0.0376 - Conf: 0.4756 -  Loss: 1.2557 - Time taken: 01:35.51\n",
      "Epoch [14/100] - Train Loss: 1.3541 - Avg IoU: 50.973% - Reg: 0.0549 - Conf: 0.4795 -  Loss: 1.4088 - Time taken: 01:35.64\n",
      "Epoch [15/100] - Train Loss: 1.3462 - Avg IoU: 44.323% - Reg: 0.0501 - Conf: 0.4765 -  Loss: 1.2915 - Time taken: 01:35.65 - Val IoU: 6.652%\n",
      "Epoch [16/100] - Train Loss: 1.3446 - Avg IoU: 43.037% - Reg: 0.0509 - Conf: 0.4756 -  Loss: 1.2737 - Time taken: 01:35.89\n",
      "Epoch [17/100] - Train Loss: 1.3424 - Avg IoU: 51.174% - Reg: 0.0672 - Conf: 0.4771 -  Loss: 1.4463 - Time taken: 01:36.46\n",
      "Epoch [18/100] - Train Loss: 1.3340 - Avg IoU: 44.755% - Reg: 0.0335 - Conf: 0.4759 -  Loss: 1.2476 - Time taken: 01:35.65\n",
      "Epoch [19/100] - Train Loss: 1.3356 - Avg IoU: 53.274% - Reg: 0.0770 - Conf: 0.4799 -  Loss: 1.5099 - Time taken: 01:36.19\n",
      "Epoch [20/100] - Train Loss: 1.3264 - Avg IoU: 43.701% - Reg: 0.0595 - Conf: 0.4794 -  Loss: 1.3133 - Time taken: 01:35.73 - Val IoU: 8.283%\n",
      "Epoch [21/100] - Train Loss: 1.3320 - Avg IoU: 47.472% - Reg: 0.0620 - Conf: 0.4789 -  Loss: 1.3770 - Time taken: 01:35.96\n",
      "Epoch [22/100] - Train Loss: 1.3203 - Avg IoU: 46.926% - Reg: 0.0382 - Conf: 0.4800 -  Loss: 1.2986 - Time taken: 01:35.81\n",
      "Epoch [23/100] - Train Loss: 1.3181 - Avg IoU: 44.228% - Reg: 0.0325 - Conf: 0.4792 -  Loss: 1.2401 - Time taken: 01:35.51\n",
      "Epoch [24/100] - Train Loss: 1.3185 - Avg IoU: 45.435% - Reg: 0.0642 - Conf: 0.4736 -  Loss: 1.3477 - Time taken: 01:35.50\n",
      "Epoch [25/100] - Train Loss: 1.3129 - Avg IoU: 44.759% - Reg: 0.0491 - Conf: 0.4733 -  Loss: 1.2919 - Time taken: 01:36.32 - Val IoU: 8.368%\n",
      "Epoch [26/100] - Train Loss: 1.3137 - Avg IoU: 46.818% - Reg: 0.0345 - Conf: 0.4733 -  Loss: 1.2790 - Time taken: 01:35.76\n",
      "Epoch [27/100] - Train Loss: 1.3116 - Avg IoU: 49.761% - Reg: 0.0632 - Conf: 0.4778 -  Loss: 1.4139 - Time taken: 01:35.91\n",
      "Epoch [28/100] - Train Loss: 1.3007 - Avg IoU: 40.767% - Reg: 0.0452 - Conf: 0.4722 -  Loss: 1.2192 - Time taken: 01:36.14\n",
      "Epoch [29/100] - Train Loss: 1.2942 - Avg IoU: 48.110% - Reg: 0.0603 - Conf: 0.4731 -  Loss: 1.3756 - Time taken: 01:35.53\n",
      "Epoch [30/100] - Train Loss: 1.2939 - Avg IoU: 46.071% - Reg: 0.0381 - Conf: 0.4743 -  Loss: 1.2796 - Time taken: 01:35.67 - Val IoU: 7.707%\n",
      "Epoch [31/100] - Train Loss: 1.3016 - Avg IoU: 46.128% - Reg: 0.0559 - Conf: 0.4799 -  Loss: 1.3396 - Time taken: 01:35.31\n",
      "Epoch [32/100] - Train Loss: 1.2878 - Avg IoU: 49.171% - Reg: 0.0997 - Conf: 0.4700 -  Loss: 1.5066 - Time taken: 01:35.59\n",
      "Epoch [33/100] - Train Loss: 1.2914 - Avg IoU: 48.376% - Reg: 0.0426 - Conf: 0.4791 -  Loss: 1.3324 - Time taken: 01:40.33\n",
      "Epoch [34/100] - Train Loss: 1.2867 - Avg IoU: 41.647% - Reg: 0.0340 - Conf: 0.4750 -  Loss: 1.2018 - Time taken: 01:27.96\n",
      "Epoch [35/100] - Train Loss: 1.2881 - Avg IoU: 45.912% - Reg: 0.0577 - Conf: 0.4804 -  Loss: 1.3423 - Time taken: 01:28.61 - Val IoU: 7.642%\n",
      "Epoch [36/100] - Train Loss: 1.2886 - Avg IoU: 44.048% - Reg: 0.0385 - Conf: 0.4738 -  Loss: 1.2500 - Time taken: 01:30.99\n",
      "Epoch [37/100] - Train Loss: 1.2868 - Avg IoU: 43.373% - Reg: 0.0656 - Conf: 0.4787 -  Loss: 1.3260 - Time taken: 01:35.30\n",
      "Epoch [38/100] - Train Loss: 1.2741 - Avg IoU: 37.343% - Reg: 0.0220 - Conf: 0.4701 -  Loss: 1.0962 - Time taken: 01:34.62\n",
      "Epoch [39/100] - Train Loss: 1.2774 - Avg IoU: 46.516% - Reg: 0.0512 - Conf: 0.4763 -  Loss: 1.3276 - Time taken: 01:35.28\n",
      "Epoch [40/100] - Train Loss: 1.2810 - Avg IoU: 44.198% - Reg: 0.0370 - Conf: 0.4773 -  Loss: 1.2513 - Time taken: 01:35.26 - Val IoU: 8.159%\n",
      "Epoch [41/100] - Train Loss: 1.2815 - Avg IoU: 42.595% - Reg: 0.0373 - Conf: 0.4708 -  Loss: 1.2215 - Time taken: 01:35.41\n",
      "Epoch [42/100] - Train Loss: 1.2766 - Avg IoU: 43.107% - Reg: 0.0259 - Conf: 0.4804 -  Loss: 1.2047 - Time taken: 01:35.07\n",
      "Epoch [43/100] - Train Loss: 1.2740 - Avg IoU: 38.059% - Reg: 0.0237 - Conf: 0.4737 -  Loss: 1.1156 - Time taken: 01:35.15\n",
      "Epoch [44/100] - Train Loss: 1.2767 - Avg IoU: 46.936% - Reg: 0.0456 - Conf: 0.4748 -  Loss: 1.3156 - Time taken: 01:33.82\n",
      "Epoch [45/100] - Train Loss: 1.2755 - Avg IoU: 43.520% - Reg: 0.0528 - Conf: 0.4746 -  Loss: 1.2857 - Time taken: 01:32.79 - Val IoU: 8.742%\n",
      "Epoch [46/100] - Train Loss: 1.2620 - Avg IoU: 41.058% - Reg: 0.0406 - Conf: 0.4750 -  Loss: 1.2127 - Time taken: 01:29.13\n",
      "Epoch [47/100] - Train Loss: 1.2632 - Avg IoU: 46.216% - Reg: 0.0472 - Conf: 0.4709 -  Loss: 1.3059 - Time taken: 01:28.94\n",
      "Epoch [48/100] - Train Loss: 1.2664 - Avg IoU: 43.613% - Reg: 0.0464 - Conf: 0.4726 -  Loss: 1.2661 - Time taken: 01:34.68\n",
      "Epoch [49/100] - Train Loss: 1.2610 - Avg IoU: 45.056% - Reg: 0.0591 - Conf: 0.4769 -  Loss: 1.3301 - Time taken: 01:35.82\n",
      "Epoch [50/100] - Train Loss: 1.2626 - Avg IoU: 40.454% - Reg: 0.0284 - Conf: 0.4773 -  Loss: 1.1693 - Time taken: 01:31.26 - Val IoU: 8.832%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 58\u001b[39m\n\u001b[32m     55\u001b[39m epoch_loss = \u001b[32m0\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m#with pytorch it wont let you pass the entire dataset to the net at once so you have to send it in batches (thats what the batch size is for)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# shape: [batch_size, 3, 224, 224]\u001b[39;49;00m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#boxes = [t['boxes'].to(device) for t in targets]  # move each image's boxes to GPU\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mx:\\Anaconda\\envs\\face-net-clean\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mx:\\Anaconda\\envs\\face-net-clean\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:788\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    787\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    790\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mx:\\Anaconda\\envs\\face-net-clean\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mx:\\Anaconda\\envs\\face-net-clean\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Code\\Neural Net Projects\\FaceNet\\utils\\import_data.py:105\u001b[39m, in \u001b[36mWiderFaceDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[32m    104\u001b[39m     sample = \u001b[38;5;28mself\u001b[39m.data[idx]\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     image = \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mimg_path\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m.convert(\u001b[33m'\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    106\u001b[39m     \u001b[38;5;66;03m#boxes = sample['boxes']\u001b[39;00m\n\u001b[32m    107\u001b[39m     boxes = sample[\u001b[33m'\u001b[39m\u001b[33mboxes\u001b[39m\u001b[33m'\u001b[39m].clone()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mx:\\Anaconda\\envs\\face-net-clean\\Lib\\site-packages\\PIL\\Image.py:3493\u001b[39m, in \u001b[36mopen\u001b[39m\u001b[34m(fp, mode, formats)\u001b[39m\n\u001b[32m   3491\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_path(fp):\n\u001b[32m   3492\u001b[39m     filename = os.fspath(fp)\n\u001b[32m-> \u001b[39m\u001b[32m3493\u001b[39m     fp = builtins.open(filename, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   3494\u001b[39m     exclusive_fp = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   3495\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#Training Loop\n",
    "start_epoch = 0\n",
    "num_epochs = 100\n",
    "loadLastCheckpoint = False\n",
    "checkpoint_rate = 10\n",
    "validationRate = 5\n",
    "anchor_sizes = [30, 60]\n",
    "warmup_epochs=10\n",
    "\n",
    "net = FaceDetectionNet(num_anchors=2)\n",
    "\n",
    "#moves all the info to the gpu (cuda) if it can, if not it keeps it on the cpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Training on: {device}\") \n",
    "net = net.to(device)\n",
    "\n",
    "# Initialize anchor generator\n",
    "anchor_gen = AnchorGeneratorMultiScale(anchor_sizes=anchor_sizes)\n",
    "anchors = anchor_gen.generate(feature_h=28, feature_w=28, stride=8, device=device)\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0001)\n",
    "scaler = GradScaler()\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min', \n",
    "    factor=0.5,\n",
    "    patience=5,      # Wait longer before giving up\n",
    "    min_lr=1e-7      # Set a floor\n",
    ")\n",
    "\n",
    "def get_lr_warmup(epoch, warmup_epochs=warmup_epochs, base_lr=0.0005):\n",
    "    if epoch < warmup_epochs:\n",
    "        return base_lr * (epoch + 1) / warmup_epochs\n",
    "    return base_lr\n",
    "\n",
    "\n",
    "if loadLastCheckpoint:\n",
    "    # Check for existing checkpoints and load the latest one\n",
    "    checkpoint_files = [f for f in os.listdir(\"checkpoints\") if f.startswith(\"faceNet_checkpoint\")]\n",
    "    if checkpoint_files:\n",
    "        # Sort by epoch number and get the latest\n",
    "        latest_checkpoint = sorted(checkpoint_files, key=lambda x: int(x.split('checkpoint')[1].split('.')[0]))[-1]\n",
    "        checkpoint_path = os.path.join(\"checkpoints\", latest_checkpoint)\n",
    "        \n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        net.load_state_dict(checkpoint[\"model_state\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
    "        start_epoch = checkpoint[\"epoch\"] + 1\n",
    "        print(f\"Loaded checkpoint from epoch {checkpoint['epoch'] + 1}\")\n",
    "\n",
    "timer_start = time.time()\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    net.train() # sets the net to training mode\n",
    "    epoch_loss = 0\n",
    "\n",
    "    #with pytorch it wont let you pass the entire dataset to the net at once so you have to send it in batches (thats what the batch size is for)\n",
    "    for images, targets in dataloader:\n",
    "        images = torch.stack(images).to(device)    # shape: [batch_size, 3, 224, 224]\n",
    "        #boxes = [t['boxes'].to(device) for t in targets]  # move each image's boxes to GPU\n",
    "\n",
    "        optimizer.zero_grad()               # resets gradients\n",
    "        with autocast('cuda'):  # autocast lets you use mixed precision for faster training on nvidia gpu, uses 16bit floating points instead of 32\n",
    "            outputs = net(images)\n",
    "            loss, avg_reg, avg_conf, avg_iou = compute_loss_single_face(outputs, targets, anchors, stride=8)\n",
    "            \n",
    "        scaler.scale(loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0) # Gradient clipping to prevent exploding gradients\n",
    "\n",
    "        # if epoch < warmup_epochs:\n",
    "        #     for param_group in optimizer.param_groups:\n",
    "        #         param_group['lr'] = get_lr_warmup(epoch)\n",
    "\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        epoch_loss += loss.item()  # accumulate batch loss\n",
    "\n",
    "    #timer to see how long each epoch takes\n",
    "    timer_end = time.time()\n",
    "    length = timer_end-timer_start\n",
    "    mins, secs = divmod(length, 60)\n",
    "\n",
    "    \n",
    "    avg_epoch_loss = epoch_loss / len(dataloader)\n",
    "    scheduler.step(avg_epoch_loss) #reduces the learning rate once the loss has stopped moving for 3 consecutive epochs\n",
    "\n",
    "    if (epoch+1) % validationRate == 0:\n",
    "        val_iou = anchor_validate_iou(net=net, val_loader=val_loader, device=device, anchors=anchors)\n",
    "        torch.cuda.empty_cache() #free up the cache on the gpu after checking iou\n",
    "\n",
    "        print(\n",
    "        f\"Epoch [{epoch+1}/{num_epochs}]\"\n",
    "        f\" - Train Loss: {avg_epoch_loss:.4f}\"\n",
    "        f\" - Avg IoU: {(avg_iou*100):.3f}%\"\n",
    "        f\" - Reg: {avg_reg:.4f}\"\n",
    "        f\" - Conf: {avg_conf:.4f}\"\n",
    "        f\" -  Loss: {loss.item():.4f}\"\n",
    "        f\" - Time taken: 0{int(mins)}:{secs:.2f}\"\n",
    "        f\" - Val IoU: {(val_iou*100):.3f}%\"\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{num_epochs}]\"\n",
    "            f\" - Train Loss: {avg_epoch_loss:.4f}\"\n",
    "            f\" - Avg IoU: {(avg_iou*100):.3f}%\"\n",
    "            f\" - Reg: {avg_reg:.4f}\"\n",
    "            f\" - Conf: {avg_conf:.4f}\"\n",
    "            f\" -  Loss: {loss.item():.4f}\"\n",
    "            f\" - Time taken: 0{int(mins)}:{secs:.2f}\")\n",
    "    \n",
    "\n",
    "    timer_start = time.time()\n",
    "\n",
    "    #saves a checkpoint\n",
    "    if (epoch+1) % checkpoint_rate == 0:\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state\": net.state_dict(),\n",
    "            \"optimizer_state\": optimizer.state_dict(),\n",
    "            \"loss\": avg_epoch_loss\n",
    "        }, f\"checkpoints/faceNet_checkpoint{epoch+1}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f178e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "anchor_validate_iou() missing 1 required positional argument: 'anchors'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# VERY IMPORTANT for inference\u001b[39;00m\n\u001b[32m     11\u001b[39m loadedNet.eval()\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m iou_avg = \u001b[43manchor_validate_iou\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloadedNet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(iou_avg*\u001b[32m100\u001b[39m)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: anchor_validate_iou() missing 1 required positional argument: 'anchors'"
     ]
    }
   ],
   "source": [
    "# Re-create the model architecture\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "loadedNet = FaceDetectionNet()\n",
    "checkpoint_path = os.path.join(\"saved_checkpoints\", \"faceNet_checkpoint60.pth\")\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "loadedNet.load_state_dict(checkpoint[\"model_state\"])\n",
    "loadedNet.to(device)\n",
    "\n",
    "# VERY IMPORTANT for inference\n",
    "loadedNet.eval()iou_avg = anchor_validate_iou(net=loadedNet, val_loader=val_loader, device=device)\n",
    "print(f\"{(iou_avg*100):.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b876332",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "\n",
    "#img = \"43_Row_Boat_Canoe_43_37.jpg\"\n",
    "#img_path = \"C:\\\\Code\\\\Neural Net Projects\\\\FaceNet\\\\Dataset\\\\WIDER_test\\\\images\\\\43--Row_Boat\\\\\" + img\n",
    "\n",
    "img = \"0_Parade_Parade_0_1020.jpg\"\n",
    "\n",
    "img_path = \"C:\\\\Code\\\\Neural Net Projects\\\\FaceNet\\\\Dataset\\\\WIDER_test\\\\images\\\\0--Parade\\\\\" + img\n",
    "def test_net(image):\n",
    "    #load image w/ preprocessing\n",
    "    \n",
    "    orig_w, orig_h = image.size  # save original size\n",
    "\n",
    "    image_tensor = TRANSFORM(image)   # same TRANSFORM as training\n",
    "    image_tensor = image_tensor.unsqueeze(0).to(device)  # [1, 3, 224, 224]\n",
    "\n",
    "    #forward pass w/ no gradient\n",
    "    with torch.no_grad():\n",
    "        outputs = loadedNet(image_tensor)\n",
    "\n",
    "    #decode the predicitions into boxes and scores\n",
    "    pred = outputs[0]  # [1, 5, 28, 28]\n",
    "\n",
    "    # Flatten\n",
    "    pred = pred.view(5, -1).permute(1, 0)  # [28*28, 5]\n",
    "\n",
    "    pred_boxes = pred[:, :4] * 224.0   # undo normalization\n",
    "    pred_scores = torch.sigmoid(pred[:, 4])\n",
    "\n",
    "    #apply confidence threshhold and nms (non max suppresion)\n",
    "    CONF_THRESH = 0.5\n",
    "    NMS_THRESH = 0.4\n",
    "\n",
    "    keep = pred_scores > CONF_THRESH\n",
    "    pred_boxes = pred_boxes[keep]\n",
    "    pred_scores = pred_scores[keep]\n",
    "\n",
    "    if pred_boxes.shape[0] > 0:\n",
    "        keep_idx = box_nms(pred_boxes, pred_scores, iou_threshold=NMS_THRESH)\n",
    "        pred_boxes = pred_boxes[keep_idx]\n",
    "        pred_scores = pred_scores[keep_idx]\n",
    "\n",
    "\n",
    "    #scale boxes back into original image\n",
    "    scale_x = orig_w / 224\n",
    "    scale_y = orig_h / 224\n",
    "\n",
    "    pred_boxes[:, [0, 2]] *= scale_x\n",
    "    pred_boxes[:, [1, 3]] *= scale_y\n",
    "\n",
    "    return pred_boxes, pred_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686bc4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "img = \"0_Parade_Parade_0_1046.jpg\"\n",
    "img_path = \"C:\\\\Code\\\\Neural Net Projects\\\\FaceNet\\\\Dataset\\\\WIDER_test\\\\images\\\\0--Parade\\\\\" + img\n",
    "image = Image.open(img_path).convert(\"RGB\")\n",
    "pred_boxes, pred_scores = test_net(image)\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.imshow(image)\n",
    "\n",
    "for box, score in zip(pred_boxes, pred_scores):\n",
    "    x1, y1, x2, y2 = box.cpu()\n",
    "    rect = patches.Rectangle(\n",
    "        (x1, y1), x2 - x1, y2 - y1,\n",
    "        linewidth=2, edgecolor='blue', facecolor='none'\n",
    "    )\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x1, y1 - 5, f\"Confidence: {score:.2f}\", color='red')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "face-net-clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
