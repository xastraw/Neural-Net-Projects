{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b89e7b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 4631\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.amp import autocast, GradScaler\n",
    "import torchvision.transforms as T\n",
    "from utils.import_data import WiderFaceDataset, TRANSFORM, TRAIN_ROOT, TRAIN_ANN_FILE, TEST_ROOT\n",
    "from utils.anchors import AnchorMatcher, AnchorGenerator, box_nms, compute_loss_with_anchors\n",
    "\n",
    "\n",
    "# Create the dataset\n",
    "dataset = WiderFaceDataset(\n",
    "    root_dir=TRAIN_ROOT, \n",
    "    annotation_file=TRAIN_ANN_FILE, \n",
    "    img_size=224, \n",
    "    transform=TRANSFORM,\n",
    "    single_face_only=True #if true, thirds the total images\n",
    "    )\n",
    "\n",
    "print(f\"Total images: {len(dataset)}\")\n",
    "\n",
    "#custom collate function to seperate the boxes from the images\n",
    "def custom_collate_fn(batch):\n",
    "    images, targets = zip(*batch)  # unzip the batch\n",
    "    return images, targets\n",
    "\n",
    "# Create a dataloader\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=64, #64 runs quicest on desktop\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    #use collate here because our data has the images but also the boxes and number of boxes\n",
    "    collate_fn = custom_collate_fn\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4d28dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDetectionNet(nn.Module):\n",
    "    def __init__(self, num_anchors=1):\n",
    "        \"\"\"\n",
    "        num_anchors: number of boxes predicted per spatial cell (simplest: 1)\n",
    "        \"\"\"\n",
    "        super(FaceDetectionNet, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        kernel_size is the size of the box we pass over each img to extract the features, exactly like tf (3,3,3)\n",
    "        \"\"\"\n",
    "        #Backbone (feature extractor)\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),  # RGB input\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # downsample by 2 -> 112x1112\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # downsample by 2 -> 56x56\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # downsample by 2 -> 28x28\n",
    "        )\n",
    "\n",
    "        # Detection head\n",
    "        # Predict bounding boxes + confidence\n",
    "        # Output channels = num_anchors * 5 (x, y, w, h, conf)\n",
    "        self.det_head = nn.Conv2d(256, num_anchors * 5, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [batch_size, 3, H, W]\n",
    "        Returns:\n",
    "            out: [batch_size, num_anchors * 5, H/4, W/4] \n",
    "                 Each cell predicts (x, y, w, h, confidence)\n",
    "        \"\"\"\n",
    "        features = self.backbone(x)\n",
    "        out = self.det_head(features)  # [B, 5*num_anchors, H', W']\n",
    "\n",
    "        B, C, H, W = out.shape\n",
    "        out = out.view(B, -1, 5, H, W)  # [B, num_anchors, 5, H', W']\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e29d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on: cuda\n",
      "Loaded checkpoint from epoch 45\n",
      "Epoch [46/100] - Average Loss: 0.1537 - Time taken: 01:4.90\n",
      "Epoch [47/100] - Average Loss: 0.1514 - Time taken: 01:4.74\n",
      "Epoch [48/100] - Average Loss: 0.1495 - Time taken: 01:4.57\n",
      "Epoch [49/100] - Average Loss: 0.1465 - Time taken: 01:4.89\n",
      "Epoch [50/100] - Average Loss: 0.1458 - Time taken: 01:8.84\n",
      "Epoch [51/100] - Average Loss: 0.1455 - Time taken: 01:9.08\n",
      "Epoch [52/100] - Average Loss: 0.1458 - Time taken: 01:8.39\n",
      "Epoch [53/100] - Average Loss: 0.1480 - Time taken: 01:8.22\n",
      "Epoch [54/100] - Average Loss: 0.1428 - Time taken: 01:8.41\n",
      "Epoch [55/100] - Average Loss: 0.1418 - Time taken: 01:8.18\n",
      "Epoch [56/100] - Average Loss: 0.1405 - Time taken: 01:8.25\n",
      "Epoch [57/100] - Average Loss: 0.1396 - Time taken: 01:8.26\n",
      "Epoch [58/100] - Average Loss: 0.1369 - Time taken: 01:8.79\n",
      "Epoch [59/100] - Average Loss: 0.1411 - Time taken: 01:8.22\n",
      "Epoch [60/100] - Average Loss: 0.1383 - Time taken: 01:8.25\n",
      "Epoch [61/100] - Average Loss: 0.1379 - Time taken: 01:8.24\n",
      "Epoch [62/100] - Average Loss: 0.1359 - Time taken: 01:8.39\n",
      "Epoch [63/100] - Average Loss: 0.1379 - Time taken: 01:8.58\n",
      "Epoch [64/100] - Average Loss: 0.1380 - Time taken: 01:8.45\n",
      "Epoch [65/100] - Average Loss: 0.1328 - Time taken: 01:8.40\n",
      "Epoch [66/100] - Average Loss: 0.1358 - Time taken: 01:8.46\n",
      "Epoch [67/100] - Average Loss: 0.1328 - Time taken: 01:8.59\n",
      "Epoch [68/100] - Average Loss: 0.1317 - Time taken: 01:8.47\n",
      "Epoch [69/100] - Average Loss: 0.1267 - Time taken: 01:8.66\n",
      "Epoch [70/100] - Average Loss: 0.1282 - Time taken: 01:8.54\n",
      "Epoch [71/100] - Average Loss: 0.1270 - Time taken: 01:8.65\n",
      "Epoch [72/100] - Average Loss: 0.1241 - Time taken: 01:7.91\n",
      "Epoch [73/100] - Average Loss: 0.1256 - Time taken: 01:8.40\n",
      "Epoch [74/100] - Average Loss: 0.1280 - Time taken: 01:8.66\n",
      "Epoch [75/100] - Average Loss: 0.1238 - Time taken: 01:8.53\n",
      "Epoch [76/100] - Average Loss: 0.1242 - Time taken: 01:8.47\n",
      "Epoch [77/100] - Average Loss: 0.1237 - Time taken: 01:8.28\n",
      "Epoch [78/100] - Average Loss: 0.1239 - Time taken: 01:8.11\n",
      "Epoch [79/100] - Average Loss: 0.1247 - Time taken: 01:8.28\n",
      "Epoch [80/100] - Average Loss: 0.1214 - Time taken: 01:8.20\n",
      "Epoch [81/100] - Average Loss: 0.1242 - Time taken: 01:8.55\n",
      "Epoch [82/100] - Average Loss: 0.1209 - Time taken: 01:8.46\n",
      "Epoch [83/100] - Average Loss: 0.1196 - Time taken: 01:8.57\n",
      "Epoch [84/100] - Average Loss: 0.1186 - Time taken: 01:8.24\n",
      "Epoch [85/100] - Average Loss: 0.1190 - Time taken: 01:9.31\n",
      "Epoch [86/100] - Average Loss: 0.1174 - Time taken: 01:8.05\n",
      "Epoch [87/100] - Average Loss: 0.1174 - Time taken: 01:8.22\n",
      "Epoch [88/100] - Average Loss: 0.1143 - Time taken: 01:8.02\n",
      "Epoch [89/100] - Average Loss: 0.1162 - Time taken: 01:8.26\n",
      "Epoch [90/100] - Average Loss: 0.1140 - Time taken: 01:8.29\n",
      "Epoch [91/100] - Average Loss: 0.1129 - Time taken: 01:7.65\n",
      "Epoch [92/100] - Average Loss: 0.1148 - Time taken: 01:7.99\n",
      "Epoch [93/100] - Average Loss: 0.1157 - Time taken: 01:8.31\n",
      "Epoch [94/100] - Average Loss: 0.1148 - Time taken: 01:8.23\n",
      "Epoch [95/100] - Average Loss: 0.1081 - Time taken: 01:8.97\n",
      "Epoch [96/100] - Average Loss: 0.1101 - Time taken: 01:8.39\n",
      "Epoch [97/100] - Average Loss: 0.1091 - Time taken: 01:7.88\n",
      "Epoch [98/100] - Average Loss: 0.1060 - Time taken: 01:8.17\n",
      "Epoch [99/100] - Average Loss: 0.1061 - Time taken: 01:8.18\n",
      "Epoch [100/100] - Average Loss: 0.1065 - Time taken: 01:8.42\n"
     ]
    }
   ],
   "source": [
    "net = FaceDetectionNet()\n",
    "\n",
    "#moves all the info to the gpu (cuda) if it can, if not it keeps it on the cpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Training on: {device}\") \n",
    "net = net.to(device)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0005)\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Initialize anchor generator and matcher\n",
    "anchor_gen = AnchorGenerator(scales=[1.0], aspect_ratios=[1.0])\n",
    "matcher = AnchorMatcher(pos_iou_thresh=0.5, neg_iou_thresh=0.4)\n",
    "# Generate anchors (assuming feature map is 28x28 after 2x2 pooling)\n",
    "anchors = anchor_gen.generate_anchors(feature_h=28, feature_w=28, stride=8, img_size=224)\n",
    "\n",
    "start_epoch = 0\n",
    "num_epochs = 100\n",
    "checkpoint_rate = 20\n",
    "\n",
    "loadLastCheckpoint = True\n",
    "if loadLastCheckpoint:\n",
    "    # Check for existing checkpoints and load the latest one\n",
    "    checkpoint_files = [f for f in os.listdir(\"checkpoints\") if f.startswith(\"faceNet_checkpoint\")]\n",
    "    if checkpoint_files:\n",
    "        # Sort by epoch number and get the latest\n",
    "        latest_checkpoint = sorted(checkpoint_files, key=lambda x: int(x.split('checkpoint')[1].split('.')[0]))[-1]\n",
    "        checkpoint_path = os.path.join(\"checkpoints\", latest_checkpoint)\n",
    "        \n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        net.load_state_dict(checkpoint[\"model_state\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
    "        start_epoch = checkpoint[\"epoch\"] + 1\n",
    "        print(f\"Loaded checkpoint from epoch {checkpoint['epoch'] + 1}\")\n",
    "\n",
    "timer_start = time.time()\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    net.train() # sets the net to training mode\n",
    "    epoch_loss = 0\n",
    "\n",
    "    #with pytorch it wont let you pass the entire dataset to the net at once so you have to send it in batches (thats what the batch size is for)\n",
    "    for images, targets in dataloader:\n",
    "        # images is a tuple of tensors; stack into a single batch tensor\n",
    "        images = torch.stack(images).to(device)    # shape: [batch_size, 3, 224, 224]\n",
    "        boxes = [t['boxes'].to(device) for t in targets]  # move each image's boxes to GPU\n",
    "\n",
    "        optimizer.zero_grad()               # resets gradients           \n",
    "\n",
    "        with autocast('cuda'):  # Use FP16 (floating point) for forward pass - faster on GPUs\n",
    "            outputs = net(images)               # forward pass\n",
    "            loss = compute_loss_with_anchors(outputs, boxes, anchors, matcher, device)\n",
    "\n",
    "        scaler.scale(loss).backward()           # backpropogation\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()                         # update weights \n",
    "        \n",
    "        epoch_loss += loss.item()  # accumulate batch loss\n",
    "\n",
    "    #timer to see how long each epoch takes\n",
    "    timer_end = time.time()\n",
    "    length = timer_end-timer_start\n",
    "    mins, secs = divmod(length, 60)\n",
    "\n",
    "    #custom loss to mimic what it looks like in tf\n",
    "    avg_epoch_loss = epoch_loss / len(dataloader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Average Loss: {avg_epoch_loss:.4f} - Time taken: 0{int(mins)}:{secs:.2f}\")\n",
    "    timer_start = time.time()\n",
    "\n",
    "    #saves a checkpoint\n",
    "    if (epoch+1) % checkpoint_rate == 0:\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state\": net.state_dict(),\n",
    "            \"optimizer_state\": optimizer.state_dict(),\n",
    "            \"loss\": avg_epoch_loss\n",
    "        }, f\"checkpoints/faceNet_checkpoint{epoch+1}.pth\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2771886a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FaceDetectionNet(\n",
       "  (backbone): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU()\n",
       "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (11): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (13): ReLU()\n",
       "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (det_head): Conv2d(256, 5, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-create the model architecture\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "loadedNet = FaceDetectionNet()\n",
    "checkpoint_path = os.path.join(\"saved_checkpoints\", \"faceNet_checkpoint100_multi.pth\")\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "loadedNet.load_state_dict(checkpoint[\"model_state\"])\n",
    "loadedNet.to(device)\n",
    "\n",
    "# VERY IMPORTANT for inference - makes sure the net isn't trying to train\n",
    "loadedNet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafbbca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def test_net(image):\n",
    "    #load image w/ preprocessing\n",
    "    \n",
    "    orig_w, orig_h = image.size  # save original size\n",
    "\n",
    "    image_tensor = TRANSFORM(image)   # same TRANSFORM as training\n",
    "    image_tensor = image_tensor.unsqueeze(0).to(device)  # [1, 3, 224, 224]\n",
    "\n",
    "    #forward pass w/ no gradient\n",
    "    with torch.no_grad():\n",
    "        outputs = loadedNet(image_tensor)\n",
    "\n",
    "    #decode the predicitions into boxes and scores\n",
    "    pred = outputs[0]  # [1, 5, 28, 28]\n",
    "\n",
    "    # Flatten\n",
    "    pred = pred.view(5, -1).permute(1, 0)  # [28*28, 5]\n",
    "\n",
    "    pred_boxes = pred[:, :4] * 224.0   # undo normalization\n",
    "    pred_scores = torch.sigmoid(pred[:, 4])\n",
    "\n",
    "    #apply confidence threshhold and nms (non max suppresion)\n",
    "    CONF_THRESH = 0.5\n",
    "    NMS_THRESH = 0.4\n",
    "\n",
    "    keep = pred_scores > CONF_THRESH\n",
    "    pred_boxes = pred_boxes[keep]\n",
    "    pred_scores = pred_scores[keep]\n",
    "\n",
    "    if pred_boxes.shape[0] > 0:\n",
    "        keep_idx = box_nms(pred_boxes, pred_scores, iou_threshold=NMS_THRESH)\n",
    "        pred_boxes = pred_boxes[keep_idx]\n",
    "        pred_scores = pred_scores[keep_idx]\n",
    "\n",
    "\n",
    "    #scale boxes back into original image\n",
    "    scale_x = orig_w / 224\n",
    "    scale_y = orig_h / 224\n",
    "\n",
    "    pred_boxes[:, [0, 2]] *= scale_x\n",
    "    pred_boxes[:, [1, 3]] *= scale_y\n",
    "\n",
    "    return pred_boxes, pred_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b82d0c11",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m img = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m img_path = \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFaceNet\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mWIDER_test\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mimages\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m0--Parade\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m0_Parade_Parade_0_1046.jpg\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m image = \u001b[43mImage\u001b[49m.open(img_path).convert(\u001b[33m\"\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m pred_boxes, pred_scores = test_net(image)\n\u001b[32m      9\u001b[39m fig, ax = plt.subplots(\u001b[32m1\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'Image' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "img = \"\"\n",
    "img_path = r\"FaceNet\\Dataset\\WIDER_test\\images\\0--Parade\\0_Parade_Parade_0_1046.jpg\"\n",
    "image = Image.open(img_path).convert(\"RGB\")\n",
    "pred_boxes, pred_scores = test_net(image)\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.imshow(image)\n",
    "\n",
    "for box, score in zip(pred_boxes, pred_scores):\n",
    "    x1, y1, x2, y2 = box.cpu()\n",
    "    rect = patches.Rectangle(\n",
    "        (x1, y1), x2 - x1, y2 - y1,\n",
    "        linewidth=2, edgecolor='blue', facecolor='none'\n",
    "    )\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x1, y1 - 5, f\"Confidence: {score:.2f}\", color='red')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "face-net-clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
