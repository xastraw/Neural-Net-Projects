{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72e0f263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train images: 4631\n",
      "Total validation images: 1122\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.amp import autocast, GradScaler\n",
    "import torchvision.transforms as T\n",
    "from utils.import_data import WiderFaceDataset, TRANSFORM, TRAIN_ROOT, TRAIN_ANN_FILE, TEST_ROOT, VAL_ROOT, VAL_ANN_FILE\n",
    "#from utils.anchors import AnchorMatcher, AnchorGenerator, box_nms, compute_loss_with_anchors\n",
    "from utils.anchors import AnchorGeneratorSingle, compute_loss_single_face, compute_iou\n",
    "\n",
    "#custom collate function to seperate the boxes from the images\n",
    "def custom_collate_fn(batch):\n",
    "    images, targets = zip(*batch)  # unzip the batch\n",
    "    return images, targets\n",
    "\n",
    "# Create the dataset\n",
    "dataset = WiderFaceDataset(\n",
    "    root_dir=TRAIN_ROOT, \n",
    "    annotation_file=TRAIN_ANN_FILE, \n",
    "    img_size=224, \n",
    "    transform=TRANSFORM,\n",
    "    single_face_only=True #if true, thirds the total images\n",
    "    )\n",
    "\n",
    "# Create a dataloader\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=64, #64 runs quickest on desktop\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    #use collate here because our data has the images but also the boxes and number of boxes\n",
    "    collate_fn = custom_collate_fn\n",
    "    )\n",
    "\n",
    "\n",
    "#load validation pictures\n",
    "val_dataset = WiderFaceDataset(\n",
    "    root_dir=VAL_ROOT, \n",
    "    annotation_file=VAL_ANN_FILE, \n",
    "    img_size=224, \n",
    "    transform=TRANSFORM,\n",
    "    single_face_only=True\n",
    "    )\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    collate_fn = custom_collate_fn\n",
    "    )\n",
    "\n",
    "print(f\"Total train images: {len(dataset)}\")\n",
    "print(f\"Total validation images: {len(val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79358bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computes the intersection over union (IoU) for a single face image. On a scale of 0-1, a higher number means the boxes are more accurate\n",
    "def anchor_validate_iou(net, val_loader, device, conf_thresh=0.05, img_size=224):\n",
    "    \"\"\"\n",
    "    IoU validation for single-face anchor-based detector.\n",
    "    Picks highest-confidence anchor per image.\n",
    "    \"\"\"\n",
    "    net.eval()\n",
    "    iou_sum = 0.0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in val_loader:\n",
    "            # Stack images and move to device\n",
    "            images = torch.stack(images).to(device)  # [B, 3, H, W]\n",
    "            outputs = net(images)                    # [B, 5, H', W']\n",
    "\n",
    "            B, C, H, W = outputs.shape\n",
    "            # Flatten spatial dimensions\n",
    "            outputs = outputs.permute(0, 2, 3, 1).reshape(B, -1, 5)  # [B, H*W, 5]\n",
    "\n",
    "            for i in range(B):\n",
    "                pred = outputs[i]               # [H*W, 5]\n",
    "\n",
    "                pred_boxes = pred[:, :4] * img_size  # scale to image size\n",
    "                pred_scores = torch.sigmoid(pred[:, 4])\n",
    "\n",
    "                # confidence filter\n",
    "                keep = pred_scores > conf_thresh\n",
    "                if keep.sum() == 0:\n",
    "                    continue\n",
    "\n",
    "                pred_boxes = pred_boxes[keep]\n",
    "                pred_scores = pred_scores[keep]\n",
    "\n",
    "                # pick best box\n",
    "                best_idx = pred_scores.argmax()\n",
    "                pred_box = pred_boxes[best_idx]\n",
    "\n",
    "                # Get GT (dataset bbx) box\n",
    "                gt_boxes = targets[i]['boxes']\n",
    "                if gt_boxes.shape[0] == 0:\n",
    "                    continue\n",
    "                gt_box = gt_boxes[0]\n",
    "\n",
    "                # Ensure 2D for compute_iou\n",
    "                iou = compute_iou(pred_box.unsqueeze(0), gt_box.unsqueeze(0))\n",
    "                iou_sum += iou.item()\n",
    "                count += 1\n",
    "\n",
    "    \n",
    "    net.train()\n",
    "    return iou_sum / max(count, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dea22ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDetectionNet(nn.Module):\n",
    "    def __init__(self, num_anchors=3):\n",
    "        \"\"\"\n",
    "        num_anchors: the number of anchor sizes (3: [32, 64, 96])\n",
    "        kernel_size is the size of the box we pass over each img to extract the features, exactly like tf (3,3,3)\n",
    "        \"\"\"\n",
    "        super(FaceDetectionNet, self).__init__()\n",
    "\n",
    "        #Backbone (feature extractor)\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),  # RGB input\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "\n",
    "        # Detection head\n",
    "        # Predict bounding boxes + confidence\n",
    "        # Output channels = num_anchors * 5 (x, y, w, h, conf)\n",
    "        self.det_head = nn.Conv2d(128, num_anchors * 5, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return self.det_head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950c2ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on: cuda\n",
      "Epoch [1/3] - Train Loss: 2.3623 - Val IoU: 0.0322% - Time taken: 01:3.53\n",
      "Epoch [2/3] - Train Loss: 1.6050 - Val IoU: 0.0000% - Time taken: 01:6.51\n",
      "Epoch [3/3] - Train Loss: 1.4351 - Val IoU: 0.0000% - Time taken: 01:7.46\n"
     ]
    }
   ],
   "source": [
    "#Training Loop\n",
    "start_epoch = 0\n",
    "num_epochs = 3\n",
    "loadLastCheckpoint = False\n",
    "checkpoint_rate = 10\n",
    "validationRate = 1\n",
    "net = FaceDetectionNet()\n",
    "\n",
    "#moves all the info to the gpu (cuda) if it can, if not it keeps it on the cpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Training on: {device}\") \n",
    "net = net.to(device)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0001)\n",
    "scaler = GradScaler()\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=3\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize anchor generator\n",
    "anchor_gen = AnchorGeneratorSingle()\n",
    "anchors = anchor_gen.generate(feature_h=28, feature_w=28, stride=8, device=device)\n",
    "\n",
    "\n",
    "if loadLastCheckpoint:\n",
    "    # Check for existing checkpoints and load the latest one\n",
    "    checkpoint_files = [f for f in os.listdir(\"checkpoints\") if f.startswith(\"faceNet_checkpoint\")]\n",
    "    if checkpoint_files:\n",
    "        # Sort by epoch number and get the latest\n",
    "        latest_checkpoint = sorted(checkpoint_files, key=lambda x: int(x.split('checkpoint')[1].split('.')[0]))[-1]\n",
    "        checkpoint_path = os.path.join(\"checkpoints\", latest_checkpoint)\n",
    "        \n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        net.load_state_dict(checkpoint[\"model_state\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
    "        start_epoch = checkpoint[\"epoch\"] + 1\n",
    "        print(f\"Loaded checkpoint from epoch {checkpoint['epoch'] + 1}\")\n",
    "\n",
    "timer_start = time.time()\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    net.train() # sets the net to training mode\n",
    "    epoch_loss = 0\n",
    "\n",
    "    #with pytorch it wont let you pass the entire dataset to the net at once so you have to send it in batches (thats what the batch size is for)\n",
    "    for images, targets in dataloader:\n",
    "        images = torch.stack(images).to(device)    # shape: [batch_size, 3, 224, 224]\n",
    "        boxes = [t['boxes'].to(device) for t in targets]  # move each image's boxes to GPU\n",
    "\n",
    "        optimizer.zero_grad()               # resets gradients\n",
    "        with autocast('cuda'):  # autocast lets you use mixed precision for faster training on nvidia gpu, uses 16bit floating points instead of 32\n",
    "            outputs = net(images)\n",
    "            loss = compute_loss_single_face(outputs, targets, anchors, stride=8)\n",
    "            \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        epoch_loss += loss.item()  # accumulate batch loss\n",
    "\n",
    "    #timer to see how long each epoch takes\n",
    "    timer_end = time.time()\n",
    "    length = timer_end-timer_start\n",
    "    mins, secs = divmod(length, 60)\n",
    "\n",
    "    \n",
    "    avg_epoch_loss = epoch_loss / len(dataloader)\n",
    "    scheduler.step(avg_epoch_loss) #reduces the learning rate once the loss has stopped moving for 3 consecutive epochs\n",
    "\n",
    "    if (epoch+1) % validationRate == 0:\n",
    "        avg_iou = anchor_validate_iou(net=net, val_loader=val_loader, device=device)\n",
    "        torch.cuda.empty_cache() #free up the cache on the gpu after checking iou\n",
    "        \n",
    "        print(\n",
    "        f\"Epoch [{epoch+1}/{num_epochs}]\"\n",
    "        f\" - Train Loss: {avg_epoch_loss:.4f}\"\n",
    "        f\" - Val IoU: {(avg_iou*100):.4f}%\"\n",
    "        f\" - Time taken: 0{int(mins)}:{secs:.2f}\")\n",
    "    else:\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{num_epochs}]\"\n",
    "            f\" - Train Loss: {avg_epoch_loss:.4f}\"\n",
    "            f\" - Time taken: 0{int(mins)}:{secs:.2f}\")\n",
    "    \n",
    "\n",
    "    timer_start = time.time()\n",
    "\n",
    "    #saves a checkpoint\n",
    "    if (epoch+1) % checkpoint_rate == 0:\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state\": net.state_dict(),\n",
    "            \"optimizer_state\": optimizer.state_dict(),\n",
    "            \"loss\": avg_epoch_loss\n",
    "        }, f\"checkpoints/faceNet_checkpoint{epoch+1}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f178e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1484\n"
     ]
    }
   ],
   "source": [
    "# Re-create the model architecture\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "loadedNet = FaceDetectionNet()\n",
    "checkpoint_path = os.path.join(\"checkpoints\", \"faceNet_checkpoint30.pth\")\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "loadedNet.load_state_dict(checkpoint[\"model_state\"])\n",
    "loadedNet.to(device)\n",
    "\n",
    "# VERY IMPORTANT for inference\n",
    "loadedNet.eval()\n",
    "iou_avg = anchor_validate_iou(net=loadedNet, val_loader=val_loader, device=device)\n",
    "print(f\"{(iou_avg*100):.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b876332",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "\n",
    "#img = \"43_Row_Boat_Canoe_43_37.jpg\"\n",
    "#img_path = \"C:\\\\Code\\\\Neural Net Projects\\\\FaceNet\\\\Dataset\\\\WIDER_test\\\\images\\\\43--Row_Boat\\\\\" + img\n",
    "\n",
    "img = \"0_Parade_Parade_0_1020.jpg\"\n",
    "\n",
    "img_path = \"C:\\\\Code\\\\Neural Net Projects\\\\FaceNet\\\\Dataset\\\\WIDER_test\\\\images\\\\0--Parade\\\\\" + img\n",
    "def test_net(image):\n",
    "    #load image w/ preprocessing\n",
    "    \n",
    "    orig_w, orig_h = image.size  # save original size\n",
    "\n",
    "    image_tensor = TRANSFORM(image)   # same TRANSFORM as training\n",
    "    image_tensor = image_tensor.unsqueeze(0).to(device)  # [1, 3, 224, 224]\n",
    "\n",
    "    #forward pass w/ no gradient\n",
    "    with torch.no_grad():\n",
    "        outputs = loadedNet(image_tensor)\n",
    "\n",
    "    #decode the predicitions into boxes and scores\n",
    "    pred = outputs[0]  # [1, 5, 28, 28]\n",
    "\n",
    "    # Flatten\n",
    "    pred = pred.view(5, -1).permute(1, 0)  # [28*28, 5]\n",
    "\n",
    "    pred_boxes = pred[:, :4] * 224.0   # undo normalization\n",
    "    pred_scores = torch.sigmoid(pred[:, 4])\n",
    "\n",
    "    #apply confidence threshhold and nms (non max suppresion)\n",
    "    CONF_THRESH = 0.5\n",
    "    NMS_THRESH = 0.4\n",
    "\n",
    "    keep = pred_scores > CONF_THRESH\n",
    "    pred_boxes = pred_boxes[keep]\n",
    "    pred_scores = pred_scores[keep]\n",
    "\n",
    "    if pred_boxes.shape[0] > 0:\n",
    "        keep_idx = box_nms(pred_boxes, pred_scores, iou_threshold=NMS_THRESH)\n",
    "        pred_boxes = pred_boxes[keep_idx]\n",
    "        pred_scores = pred_scores[keep_idx]\n",
    "\n",
    "\n",
    "    #scale boxes back into original image\n",
    "    scale_x = orig_w / 224\n",
    "    scale_y = orig_h / 224\n",
    "\n",
    "    pred_boxes[:, [0, 2]] *= scale_x\n",
    "    pred_boxes[:, [1, 3]] *= scale_y\n",
    "\n",
    "    return pred_boxes, pred_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686bc4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "img = \"0_Parade_Parade_0_1046.jpg\"\n",
    "img_path = \"C:\\\\Code\\\\Neural Net Projects\\\\FaceNet\\\\Dataset\\\\WIDER_test\\\\images\\\\0--Parade\\\\\" + img\n",
    "image = Image.open(img_path).convert(\"RGB\")\n",
    "pred_boxes, pred_scores = test_net(image)\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.imshow(image)\n",
    "\n",
    "for box, score in zip(pred_boxes, pred_scores):\n",
    "    x1, y1, x2, y2 = box.cpu()\n",
    "    rect = patches.Rectangle(\n",
    "        (x1, y1), x2 - x1, y2 - y1,\n",
    "        linewidth=2, edgecolor='blue', facecolor='none'\n",
    "    )\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x1, y1 - 5, f\"Confidence: {score:.2f}\", color='red')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "face-net-clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
