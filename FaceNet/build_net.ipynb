{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89e7b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 4631\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.amp import autocast, GradScaler\n",
    "# Importing the class to get the dataset\n",
    "from utils.import_data import WiderFaceDataset, TRANSFORM, TRAIN_ROOT, ANN_FILE\n",
    "from utils.anchors import AnchorMatcher, AnchorGenerator, box_nms, compute_loss_with_anchors\n",
    "\n",
    "\n",
    "# Create the dataset\n",
    "dataset = WiderFaceDataset(\n",
    "    root_dir=TRAIN_ROOT, \n",
    "    annotation_file=ANN_FILE, \n",
    "    img_size=224, \n",
    "    transform=TRANSFORM,\n",
    "    single_face_only=True #if true, thirds the total images\n",
    "    )\n",
    "\n",
    "print(f\"Total images: {len(dataset)}\")\n",
    "\n",
    "#custom collate function to seperate the boxes from the images\n",
    "def custom_collate_fn(batch):\n",
    "    images, targets = zip(*batch)  # unzip the batch\n",
    "    return images, targets\n",
    "\n",
    "# Create a dataloader\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    #use collate here because our data has the images but also the boxes and number of boxes\n",
    "    collate_fn = custom_collate_fn\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4d28dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDetectionNet(nn.Module):\n",
    "    def __init__(self, num_anchors=1):\n",
    "        \"\"\"\n",
    "        num_anchors: number of boxes predicted per spatial cell (simplest: 1)\n",
    "        \"\"\"\n",
    "        super(FaceDetectionNet, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        kernel_size is the size of the box we pass over each img to extract the features, exactly like tf (3,3,3)\n",
    "        \"\"\"\n",
    "        #Backbone (feature extractor)\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),  # RGB input\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # downsample by 2 -> 112x1112\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # downsample by 2 -> 56x56\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # downsample by 2 -> 28x28\n",
    "        )\n",
    "\n",
    "        # Detection head\n",
    "        # Predict bounding boxes + confidence\n",
    "        # Output channels = num_anchors * 5 (x, y, w, h, conf)\n",
    "        self.det_head = nn.Conv2d(256, num_anchors * 5, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [batch_size, 3, H, W]\n",
    "        Returns:\n",
    "            out: [batch_size, num_anchors * 5, H/4, W/4] \n",
    "                 Each cell predicts (x, y, w, h, confidence)\n",
    "        \"\"\"\n",
    "        features = self.backbone(x)\n",
    "        out = self.det_head(features)  # [B, 5*num_anchors, H', W']\n",
    "\n",
    "        B, C, H, W = out.shape\n",
    "        out = out.view(B, -1, 5, H, W)  # [B, num_anchors, 5, H', W']\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5e29d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on: cuda\n",
      "Epoch [1/3] - Average Loss: 0.3115\n",
      "Epoch [2/3] - Average Loss: 0.2035\n",
      "Epoch [3/3] - Average Loss: 0.2052\n"
     ]
    }
   ],
   "source": [
    "net = FaceDetectionNet()\n",
    "\n",
    "#moves all the info to the gpu (cuda) if it can, if not it keeps it on the cpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Training on: {device}\") \n",
    "net = net.to(device)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0005)\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Initialize anchor generator and matcher\n",
    "anchor_gen = AnchorGenerator(scales=[1.0], aspect_ratios=[1.0])\n",
    "matcher = AnchorMatcher(pos_iou_thresh=0.5, neg_iou_thresh=0.4)\n",
    "\n",
    "# Generate anchors (assuming feature map is 28x28 after 2x2 pooling)\n",
    "anchors = anchor_gen.generate_anchors(feature_h=28, feature_w=28, stride=8, img_size=224)\n",
    "\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    net.train() # sets the net to training mode\n",
    "    epoch_loss = 0\n",
    "\n",
    "    #with pytorch it wont let you pass the entire dataset to the net at once so you have to send it in batches (thats what the batch size is for)\n",
    "    for images, targets in dataloader:\n",
    "        # images is a tuple of tensors; stack into a single batch tensor\n",
    "        images = torch.stack(images).to(device)    # shape: [batch_size, 3, 224, 224]\n",
    "        boxes = [t['boxes'].to(device) for t in targets]  # move each image's boxes to GPU\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()               # resets gradients\n",
    "        \n",
    "        # outputs = net(images)               # forward pass\n",
    "\n",
    "        # loss = compute_loss_with_anchors(outputs, boxes, anchors, matcher, device)\n",
    "        # loss.backward()                     # backpropogation\n",
    "        # optimizer.step()                    # update weights \n",
    "\n",
    "        with autocast('cuda'):  # Use FP16 for forward pass\n",
    "            outputs = net(images)\n",
    "            loss = compute_loss_with_anchors(outputs, boxes, anchors, matcher, device)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        epoch_loss += loss.item()  # accumulate batch loss\n",
    "\n",
    "    #custom loss to mimic what it looks like in tf\n",
    "    avg_epoch_loss = epoch_loss / len(dataloader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Average Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3c09e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference with NMS\n",
    "def detect_faces(image, model, anchors, conf_threshold=0.5, nms_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Detect faces in image using trained model\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image.unsqueeze(0))  # [1, 1, 5, H, W]\n",
    "        \n",
    "        pred_flat = outputs[0].view(5, -1).T  # [num_anchors, 5]\n",
    "        pred_boxes = pred_flat[:, :4]\n",
    "        pred_conf = torch.sigmoid(pred_flat[:, 4])\n",
    "        \n",
    "        # Filter by confidence\n",
    "        keep_idx = pred_conf > conf_threshold\n",
    "        boxes = anchors[keep_idx]\n",
    "        scores = pred_conf[keep_idx]\n",
    "        \n",
    "        # Apply NMS\n",
    "        keep = box_nms(boxes, scores, nms_threshold)\n",
    "        \n",
    "        return boxes[keep], scores[keep]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "face-net-clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
