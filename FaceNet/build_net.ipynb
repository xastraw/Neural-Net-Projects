{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b89e7b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 4631\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.amp import autocast, GradScaler\n",
    "# Importing the class to get the dataset\n",
    "from utils.import_data import WiderFaceDataset, TRANSFORM, TRAIN_ROOT, TRAIN_ANN_FILE, TEST_ROOT\n",
    "from utils.anchors import AnchorMatcher, AnchorGenerator, box_nms, compute_loss_with_anchors\n",
    "\n",
    "\n",
    "# Create the dataset\n",
    "dataset = WiderFaceDataset(\n",
    "    root_dir=TRAIN_ROOT, \n",
    "    annotation_file=TRAIN_ANN_FILE, \n",
    "    img_size=224, \n",
    "    transform=TRANSFORM,\n",
    "    single_face_only=True #if true, thirds the total images\n",
    "    )\n",
    "\n",
    "print(f\"Total images: {len(dataset)}\")\n",
    "\n",
    "#custom collate function to seperate the boxes from the images\n",
    "def custom_collate_fn(batch):\n",
    "    images, targets = zip(*batch)  # unzip the batch\n",
    "    return images, targets\n",
    "\n",
    "# Create a dataloader\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=64, #64 runs quicest on desktop\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    #use collate here because our data has the images but also the boxes and number of boxes\n",
    "    collate_fn = custom_collate_fn\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c4d28dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDetectionNet(nn.Module):\n",
    "    def __init__(self, num_anchors=1):\n",
    "        \"\"\"\n",
    "        num_anchors: number of boxes predicted per spatial cell (simplest: 1)\n",
    "        \"\"\"\n",
    "        super(FaceDetectionNet, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        kernel_size is the size of the box we pass over each img to extract the features, exactly like tf (3,3,3)\n",
    "        \"\"\"\n",
    "        #Backbone (feature extractor)\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),  # RGB input\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # downsample by 2 -> 112x1112\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # downsample by 2 -> 56x56\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # downsample by 2 -> 28x28\n",
    "        )\n",
    "\n",
    "        # Detection head\n",
    "        # Predict bounding boxes + confidence\n",
    "        # Output channels = num_anchors * 5 (x, y, w, h, conf)\n",
    "        self.det_head = nn.Conv2d(256, num_anchors * 5, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [batch_size, 3, H, W]\n",
    "        Returns:\n",
    "            out: [batch_size, num_anchors * 5, H/4, W/4] \n",
    "                 Each cell predicts (x, y, w, h, confidence)\n",
    "        \"\"\"\n",
    "        features = self.backbone(x)\n",
    "        out = self.det_head(features)  # [B, 5*num_anchors, H', W']\n",
    "\n",
    "        B, C, H, W = out.shape\n",
    "        out = out.view(B, -1, 5, H, W)  # [B, num_anchors, 5, H', W']\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5e29d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on: cuda\n",
      "Loaded checkpoint from epoch 45\n",
      "Epoch [46/100] - Average Loss: 0.1537 - Time taken: 01:4.90\n",
      "Epoch [47/100] - Average Loss: 0.1514 - Time taken: 01:4.74\n",
      "Epoch [48/100] - Average Loss: 0.1495 - Time taken: 01:4.57\n",
      "Epoch [49/100] - Average Loss: 0.1465 - Time taken: 01:4.89\n",
      "Epoch [50/100] - Average Loss: 0.1458 - Time taken: 01:8.84\n",
      "Epoch [51/100] - Average Loss: 0.1455 - Time taken: 01:9.08\n",
      "Epoch [52/100] - Average Loss: 0.1458 - Time taken: 01:8.39\n",
      "Epoch [53/100] - Average Loss: 0.1480 - Time taken: 01:8.22\n",
      "Epoch [54/100] - Average Loss: 0.1428 - Time taken: 01:8.41\n",
      "Epoch [55/100] - Average Loss: 0.1418 - Time taken: 01:8.18\n",
      "Epoch [56/100] - Average Loss: 0.1405 - Time taken: 01:8.25\n",
      "Epoch [57/100] - Average Loss: 0.1396 - Time taken: 01:8.26\n",
      "Epoch [58/100] - Average Loss: 0.1369 - Time taken: 01:8.79\n",
      "Epoch [59/100] - Average Loss: 0.1411 - Time taken: 01:8.22\n",
      "Epoch [60/100] - Average Loss: 0.1383 - Time taken: 01:8.25\n",
      "Epoch [61/100] - Average Loss: 0.1379 - Time taken: 01:8.24\n",
      "Epoch [62/100] - Average Loss: 0.1359 - Time taken: 01:8.39\n",
      "Epoch [63/100] - Average Loss: 0.1379 - Time taken: 01:8.58\n",
      "Epoch [64/100] - Average Loss: 0.1380 - Time taken: 01:8.45\n",
      "Epoch [65/100] - Average Loss: 0.1328 - Time taken: 01:8.40\n",
      "Epoch [66/100] - Average Loss: 0.1358 - Time taken: 01:8.46\n",
      "Epoch [67/100] - Average Loss: 0.1328 - Time taken: 01:8.59\n",
      "Epoch [68/100] - Average Loss: 0.1317 - Time taken: 01:8.47\n",
      "Epoch [69/100] - Average Loss: 0.1267 - Time taken: 01:8.66\n",
      "Epoch [70/100] - Average Loss: 0.1282 - Time taken: 01:8.54\n",
      "Epoch [71/100] - Average Loss: 0.1270 - Time taken: 01:8.65\n",
      "Epoch [72/100] - Average Loss: 0.1241 - Time taken: 01:7.91\n",
      "Epoch [73/100] - Average Loss: 0.1256 - Time taken: 01:8.40\n",
      "Epoch [74/100] - Average Loss: 0.1280 - Time taken: 01:8.66\n",
      "Epoch [75/100] - Average Loss: 0.1238 - Time taken: 01:8.53\n",
      "Epoch [76/100] - Average Loss: 0.1242 - Time taken: 01:8.47\n",
      "Epoch [77/100] - Average Loss: 0.1237 - Time taken: 01:8.28\n",
      "Epoch [78/100] - Average Loss: 0.1239 - Time taken: 01:8.11\n",
      "Epoch [79/100] - Average Loss: 0.1247 - Time taken: 01:8.28\n",
      "Epoch [80/100] - Average Loss: 0.1214 - Time taken: 01:8.20\n",
      "Epoch [81/100] - Average Loss: 0.1242 - Time taken: 01:8.55\n",
      "Epoch [82/100] - Average Loss: 0.1209 - Time taken: 01:8.46\n",
      "Epoch [83/100] - Average Loss: 0.1196 - Time taken: 01:8.57\n",
      "Epoch [84/100] - Average Loss: 0.1186 - Time taken: 01:8.24\n",
      "Epoch [85/100] - Average Loss: 0.1190 - Time taken: 01:9.31\n",
      "Epoch [86/100] - Average Loss: 0.1174 - Time taken: 01:8.05\n",
      "Epoch [87/100] - Average Loss: 0.1174 - Time taken: 01:8.22\n",
      "Epoch [88/100] - Average Loss: 0.1143 - Time taken: 01:8.02\n",
      "Epoch [89/100] - Average Loss: 0.1162 - Time taken: 01:8.26\n",
      "Epoch [90/100] - Average Loss: 0.1140 - Time taken: 01:8.29\n",
      "Epoch [91/100] - Average Loss: 0.1129 - Time taken: 01:7.65\n",
      "Epoch [92/100] - Average Loss: 0.1148 - Time taken: 01:7.99\n",
      "Epoch [93/100] - Average Loss: 0.1157 - Time taken: 01:8.31\n",
      "Epoch [94/100] - Average Loss: 0.1148 - Time taken: 01:8.23\n",
      "Epoch [95/100] - Average Loss: 0.1081 - Time taken: 01:8.97\n",
      "Epoch [96/100] - Average Loss: 0.1101 - Time taken: 01:8.39\n",
      "Epoch [97/100] - Average Loss: 0.1091 - Time taken: 01:7.88\n",
      "Epoch [98/100] - Average Loss: 0.1060 - Time taken: 01:8.17\n",
      "Epoch [99/100] - Average Loss: 0.1061 - Time taken: 01:8.18\n",
      "Epoch [100/100] - Average Loss: 0.1065 - Time taken: 01:8.42\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "net = FaceDetectionNet()\n",
    "\n",
    "#moves all the info to the gpu (cuda) if it can, if not it keeps it on the cpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Training on: {device}\") \n",
    "net = net.to(device)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0005)\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Initialize anchor generator and matcher\n",
    "anchor_gen = AnchorGenerator(scales=[1.0], aspect_ratios=[1.0])\n",
    "matcher = AnchorMatcher(pos_iou_thresh=0.5, neg_iou_thresh=0.4)\n",
    "# Generate anchors (assuming feature map is 28x28 after 2x2 pooling)\n",
    "anchors = anchor_gen.generate_anchors(feature_h=28, feature_w=28, stride=8, img_size=224)\n",
    "\n",
    "start_epoch = 0\n",
    "num_epochs = 100\n",
    "checkpoint_rate = 20\n",
    "\n",
    "loadLastCheckpoint = True\n",
    "if loadLastCheckpoint:\n",
    "    # Check for existing checkpoints and load the latest one\n",
    "    checkpoint_files = [f for f in os.listdir(\"checkpoints\") if f.startswith(\"faceNet_checkpoint\")]\n",
    "    if checkpoint_files:\n",
    "        # Sort by epoch number and get the latest\n",
    "        latest_checkpoint = sorted(checkpoint_files, key=lambda x: int(x.split('checkpoint')[1].split('.')[0]))[-1]\n",
    "        checkpoint_path = os.path.join(\"checkpoints\", latest_checkpoint)\n",
    "        \n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        net.load_state_dict(checkpoint[\"model_state\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
    "        start_epoch = checkpoint[\"epoch\"] + 1\n",
    "        print(f\"Loaded checkpoint from epoch {checkpoint['epoch'] + 1}\")\n",
    "\n",
    "timer_start = time.time()\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    net.train() # sets the net to training mode\n",
    "    epoch_loss = 0\n",
    "\n",
    "    #with pytorch it wont let you pass the entire dataset to the net at once so you have to send it in batches (thats what the batch size is for)\n",
    "    for images, targets in dataloader:\n",
    "        # images is a tuple of tensors; stack into a single batch tensor\n",
    "        images = torch.stack(images).to(device)    # shape: [batch_size, 3, 224, 224]\n",
    "        boxes = [t['boxes'].to(device) for t in targets]  # move each image's boxes to GPU\n",
    "\n",
    "        optimizer.zero_grad()               # resets gradients\n",
    "        # outputs = net(images)               # forward pass\n",
    "        # loss = compute_loss_with_anchors(outputs, boxes, anchors, matcher, device)\n",
    "        # loss.backward()                     # backpropogation\n",
    "        # optimizer.step()                    # update weights \n",
    "\n",
    "        with autocast('cuda'):  # Use FP16 for forward pass\n",
    "            outputs = net(images)\n",
    "            loss = compute_loss_with_anchors(outputs, boxes, anchors, matcher, device)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        epoch_loss += loss.item()  # accumulate batch loss\n",
    "\n",
    "    #timer to see how long each epoch takes\n",
    "    timer_end = time.time()\n",
    "    length = timer_end-timer_start\n",
    "    mins, secs = divmod(length, 60)\n",
    "\n",
    "    #custom loss to mimic what it looks like in tf\n",
    "    avg_epoch_loss = epoch_loss / len(dataloader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Average Loss: {avg_epoch_loss:.4f} - Time taken: 0{int(mins)}:{secs:.2f}\")\n",
    "    timer_start = time.time()\n",
    "\n",
    "    #saves a checkpoint\n",
    "    if (epoch+1) % checkpoint_rate == 0:\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state\": net.state_dict(),\n",
    "            \"optimizer_state\": optimizer.state_dict(),\n",
    "            \"loss\": avg_epoch_loss\n",
    "        }, f\"checkpoints/faceNet_checkpoint{epoch+1}.pth\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2771886a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FaceDetectionNet(\n",
       "  (backbone): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU()\n",
       "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (11): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (13): ReLU()\n",
       "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (det_head): Conv2d(256, 5, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-create the model architecture\n",
    "net = FaceDetectionNet()\n",
    "checkpoint_path = os.path.join(\"checkpoints\", \"faceNet_checkpoint100.pth\")\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "net.load_state_dict(checkpoint[\"model_state\"])\n",
    "net.to(device)\n",
    "\n",
    "# VERY IMPORTANT for inference\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1bc9aa1",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "indices should be either on cpu or on the same device as the indexed tensor (cpu)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     22\u001b[39m image_tensor = transform(original_image).to(device)\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Run detection\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m boxes, scores = \u001b[43mdetect_faces\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manchors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnms_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(boxes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m faces\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(boxes) > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mdetect_faces\u001b[39m\u001b[34m(image, model, anchors, conf_threshold, nms_threshold)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Filter by confidence\u001b[39;00m\n\u001b[32m     15\u001b[39m keep_idx = pred_conf > conf_threshold\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m boxes = \u001b[43manchors\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkeep_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     17\u001b[39m scores = pred_conf[keep_idx]\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Apply NMS\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: indices should be either on cpu or on the same device as the indexed tensor (cpu)"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Load a test image\n",
    "image_path = r\"C:\\Code\\Neural Net Projects\\FaceNet\\Dataset\\WIDER_test\\images\\0--Parade\\0_Parade_Parade_0_1007.jpg\"\n",
    "original_image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "# Get original image dimensions for scaling boxes back\n",
    "orig_width, orig_height = original_image.size\n",
    "\n",
    "# Preprocess image for model (224x224)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "device = 'cuda'\n",
    "image_tensor = transform(original_image).to(device)\n",
    "\n",
    "# Run detection\n",
    "boxes, scores = detect_faces(image_tensor, net, anchors, conf_threshold=0.5, nms_threshold=0.5)\n",
    "\n",
    "print(f\"Found {len(boxes)} faces\")\n",
    "if len(boxes) > 0:\n",
    "    print(f\"Boxes (in 224x224 space):\\n{boxes}\")\n",
    "    print(f\"Confidence Scores:\\n{scores}\")\n",
    "\n",
    "# Scale boxes back to original image dimensions\n",
    "if len(boxes) > 0:\n",
    "    scale_x = orig_width / 224\n",
    "    scale_y = orig_height / 224\n",
    "    boxes_scaled = boxes.clone()\n",
    "    boxes_scaled[:, [0, 2]] *= scale_x\n",
    "    boxes_scaled[:, [1, 3]] *= scale_y\n",
    "else:\n",
    "    boxes_scaled = boxes\n",
    "\n",
    "# Visualize on original image\n",
    "fig, ax = plt.subplots(1, figsize=(12, 12))\n",
    "ax.imshow(original_image)\n",
    "\n",
    "for i, box in enumerate(boxes_scaled):\n",
    "    x_min, y_min, x_max, y_max = box[:4].cpu().numpy()\n",
    "    score = scores[i].cpu().item()\n",
    "    \n",
    "    # Draw rectangle\n",
    "    width = x_max - x_min\n",
    "    height = y_max - y_min\n",
    "    rect = patches.Rectangle((x_min, y_min), width, height, linewidth=2, edgecolor='red', facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "    \n",
    "    # Add confidence label\n",
    "    ax.text(x_min, y_min - 10, f'Confidence: {score:.3f}', color='red', fontsize=11, \n",
    "            bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "ax.set_title(f\"Face Detection Results - Found {len(boxes)} faces\", fontsize=14)\n",
    "ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3c09e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference with NMS\n",
    "def detect_faces(image, model, anchors, conf_threshold=0.5, nms_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Detect faces in image using trained model\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image.unsqueeze(0))  # [1, 1, 5, H, W]\n",
    "        \n",
    "        pred_flat = outputs[0].view(5, -1).T  # [num_anchors, 5]\n",
    "        pred_boxes = pred_flat[:, :4]\n",
    "        pred_conf = torch.sigmoid(pred_flat[:, 4])\n",
    "        \n",
    "        # Filter by confidence\n",
    "        keep_idx = pred_conf > conf_threshold\n",
    "        boxes = anchors[keep_idx]\n",
    "        scores = pred_conf[keep_idx]\n",
    "        \n",
    "        # Apply NMS\n",
    "        keep = box_nms(boxes, scores, nms_threshold)\n",
    "        \n",
    "        return boxes[keep], scores[keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0861714",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Load image\n",
    "image_path = r\"C:\\Code\\Neural Net Projects\\FaceNet\\Dataset\\WIDER_test\\images\\0--Parade\\0_Parade_Parade_0_1007.jpg\"  # Replace with your image\n",
    "#image_path = TEST_ROOT + \"0_Parade_Parade_0_1046.jpg\"\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "# Preprocess image (same as training)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "device = 'cpu'\n",
    "image_tensor = transform(image).to(device)\n",
    "\n",
    "# Run detection\n",
    "boxes, scores = detect_faces(image_tensor, net, anchors, conf_threshold=0.5, nms_threshold=0.5)\n",
    "\n",
    "print(f\"Found {len(boxes)} faces\")\n",
    "if len(boxes) > 0:\n",
    "    print(f\"Boxes:\\n{boxes}\")\n",
    "    print(f\"Scores:\\n{scores}\")\n",
    "\n",
    "# Visualize results\n",
    "fig, ax = plt.subplots(1, figsize=(10, 10))\n",
    "ax.imshow(image)\n",
    "\n",
    "for i, box in enumerate(boxes):\n",
    "    x, y, w, h = box[:4].cpu().numpy()\n",
    "    score = scores[i].cpu().item()\n",
    "    \n",
    "    rect = patches.Rectangle((x, y), w, h, linewidth=2, edgecolor='red', facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x, y-5, f'{score:.2f}', color='red', fontsize=10)\n",
    "\n",
    "ax.set_title(f\"Detected {len(boxes)} faces\")\n",
    "ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "face-net-clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
